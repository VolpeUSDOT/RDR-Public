{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6059d699",
   "metadata": {},
   "source": [
    "# Equity Analysis at the TAZ Level\n",
    "This Jupyter notebook takes AequilibraE runs (with and without resilience investment) and outputs an HTML file that reports changes in metrics by equity category. \n",
    "\n",
    "The default assumption is that the user will run the equity overlay analysis (`run_equity_overlay.bat` file in C:\\GitHub\\RDR\\helper_tools\\equity_analysis) as a first step, and then use the output from that as an input to this TAZ metrics analysis. However, the user may also directly provide data in a CSV file assigning an equity variable value to each TAZ from another source, rather than running the equity overlay analysis. If providing other data, the equity data must be numeric. Consult the RDR User Guide in C:\\GitHub\\RDR\\documentation for more information on how to use and understand this tool.\n",
    "\n",
    "The purpose is to help the user examine and understand differential impacts of a resilience investment intended to mitigate effects of a disruption, comparing different equity categories of interest. The analysis displays variables to help illuminate the following questions from various angles.\n",
    "\n",
    "Questions driving this analysis include:\n",
    "- What is the baseline magnitude of trips, minutes per trip, and miles per trip for each equity category?\n",
    "- How relevant is the disruption for each equity category?\n",
    "- What is the projected impact of the resilience investment overall and for each equity category, i.e., are the benefits equitably distributed?\n",
    "\n",
    "The `equity_metrics.config` configuration file allows the user to specify the following:\n",
    "- `path_to_RDR_config_file` â€“ This should identify the location of the configuration file pertinent to the existing RDR Metamodel run and corresponding AequilibraE runs that will be used for this equity analysis. The analysis will use this configuration file to identify where to access the OMX files from those runs.\n",
    "- `resil` - Resilience project.\n",
    "- `hazard`, `recovery`, `socio`, `proj_group`, `elasticity`, `run_type` - Aequilibrae scenario dimensions.\n",
    "- *Note*: As described above, the default assumption is that the user will use the equity overlay analysis first, and then use the output from that as an input to this TAZ metrics analysis. If the user will instead directly provide the equity data then the user should update the `equity_metrics.config` file (or renamed config file referenced in the run_TAZ_metrics.bat file, if applicable) to specify the name of the user-provided file in `output_name` (without the CSV file extension). The equity data must be numeric.\n",
    "    \n",
    "## Check output directory for CSV file outputs with underlying data results. \n",
    "This is in the same location as this HTML file, and is the directory specified in the `equity_analysis_dir` parameter in the equity metrics config file.\n",
    "\n",
    "## Scroll down in this HTML file for charts and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openmatrix as omx\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import equity_config_reader\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../../metamodel_py'))\n",
    "import rdr_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The equity configuration filepath is now passed into this notebook from its parent, TAZ_metrics.py, via the temporary text file,\n",
    "# assuming this notebook and TAZ_metrics.py are both in the same folder.\n",
    "# To run the notebook in isolation, rather than by executing the run_TAZ_metrics.bat file, comment out the below three lines and \n",
    "# uncomment the subsequent two lines.\n",
    "with open('temp.txt', 'r') as f:\n",
    "    config_filepath = f.read()\n",
    "equity_cfg = equity_config_reader.read_equity_config_file(config_filepath)\n",
    "\n",
    "#config_filepath = \"C:\\GitHub\\RDR\\helper_tools\\equity_analysis\\equity_metrics.config\"\n",
    "#equity_cfg = equity_config_reader.read_equity_config_file(config_filepath)\n",
    "\n",
    "rdr_cfg_path = equity_cfg['path_to_RDR_config_file']\n",
    "\n",
    "# Directory of equity helper tool files\n",
    "equity_dir = equity_cfg['equity_analysis_dir']\n",
    "\n",
    "cfg = rdr_setup.read_config_file(rdr_cfg_path)\n",
    "\n",
    "RDR_run_id = cfg['run_id']\n",
    "\n",
    "# Name of equity variable\n",
    "category_name = equity_cfg['equity_feature']\n",
    "\n",
    "# Name of CSV file with equity category value for each TAZ (either output from run_equity_overlay.bat OR user-provided)\n",
    "category_filename = equity_cfg['output_name']\n",
    "\n",
    "# P-value for use in statistical tests\n",
    "pval = float(equity_cfg['pval'])\n",
    "\n",
    "# Look to see if the equity overlay data exists\n",
    "if not os.path.exists(os.path.join(equity_dir, category_filename + '.csv')):\n",
    "    print('{}.csv not found in {}. Please run the equity_overlay first or generate your own file and specify the filename for it as output_name in the equity_metrics.config file'.format(category_filename, equity_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c964bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility method for reading OMX files\n",
    "def readOMX(filename, selectedMatrix, debug_mode):\n",
    "    f = omx.open_file(filename)\n",
    "    matrix_size = f.shape()\n",
    "    if debug_mode:\n",
    "        print('Shape: ', f.shape())\n",
    "        print('Number of tables: ', len(f))\n",
    "        print('Table names: ', f.list_matrices())\n",
    "        print('Attributes: ', f.list_all_attributes())\n",
    "    omx_df = f[selectedMatrix]\n",
    "    if debug_mode:\n",
    "        print('Sum of matrix elements: ', '{:.9}'.format(np.sum(omx_df)))\n",
    "        print('Percentiles: ', np.percentile(omx_df, (1, 10, 30, 50, 70, 90, 99)))\n",
    "        print('Maximum: ', np.amax(omx_df))\n",
    "    return omx_df, matrix_size, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for comparison - these come from equity_cfg, the equity_metrics.config file\n",
    "resil = equity_cfg['resil']\n",
    "baseline = equity_cfg['baseline']\n",
    "hazard = equity_cfg['hazard']\n",
    "recovery = equity_cfg['recovery']\n",
    "socio = equity_cfg['socio']\n",
    "projgroup = equity_cfg['projgroup']\n",
    "elasticity = equity_cfg['elasticity']\n",
    "elasname = str(int(10 * -elasticity))\n",
    "run_type = equity_cfg['run_type']\n",
    "largeval = float(equity_cfg['largeval'])\n",
    "\n",
    "hours_name = 'free_flow_time'\n",
    "miles_name = 'distance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the \"matrix\" OMX files for \"base\"\n",
    "matrix_omx_folder_path_base = os.path.join(equity_dir, \"aeq_runs\", \"base\", RDR_run_id,\n",
    "                                           socio + projgroup, \"matrix\", \"matrices\")\n",
    "\n",
    "# Location of the \"nocar\" OMX files for \"base\"\n",
    "nocar_omx_folder_path_base = os.path.join(equity_dir, \"aeq_runs\", \"base\", RDR_run_id,\n",
    "                                          socio + projgroup, \"nocar\", \"matrices\")\n",
    "\n",
    "# Location of the \"matrix\" OMX files for \"disruption with resilience investment\"\n",
    "matrix_omx_folder_path = os.path.join(equity_dir, \"aeq_runs\", \"disrupt\", RDR_run_id,\n",
    "                             socio + projgroup + '_' + resil + '_' + elasname + '_' + hazard + '_' + recovery,\n",
    "                             \"matrix\", \"matrices\")\n",
    "\n",
    "# Location of the \"matrix\" OMX files for \"disruption WITHOUT resilience investment\"\n",
    "matrix_omx_folder_path_noresil = os.path.join(equity_dir, \"aeq_runs\", \"disrupt\", RDR_run_id,\n",
    "                                     socio + projgroup + '_' + baseline + '_' + elasname + '_' + hazard + '_' + recovery,\n",
    "                                     \"matrix\", \"matrices\")\n",
    "\n",
    "# Location of the \"nocar\" OMX files for \"disruption with resilience investment\"\n",
    "nocar_omx_folder_path = os.path.join(equity_dir, \"aeq_runs\", \"disrupt\", RDR_run_id,\n",
    "                             socio + projgroup + '_' + resil + '_' + elasname + '_' + hazard + '_' + recovery,\n",
    "                             \"nocar\", \"matrices\")\n",
    "\n",
    "# Location of the \"nocar\" OMX files for \"disruption WITHOUT resilience investment\"\n",
    "nocar_omx_folder_path_noresil = os.path.join(equity_dir, \"aeq_runs\", \"disrupt\", RDR_run_id,\n",
    "                                     socio + projgroup + '_' + baseline + '_' + elasname + '_' + hazard + '_' + recovery,\n",
    "                                     \"nocar\", \"matrices\")\n",
    "\n",
    "# READING THE TABLES FOR \"MATRIX\"\n",
    "\n",
    "# Read the base OMX trip table\n",
    "matrix_base_matrix_filename = os.path.join(matrix_omx_folder_path_base, 'base_demand_summed.omx')\n",
    "matrix_base_dem, matrix_base_trips_matrix_size, matrix_base_trip_omx_file = readOMX(matrix_base_matrix_filename, 'matrix', 0)\n",
    "df_matrix_base_trips = pd.DataFrame(data=matrix_base_dem)\n",
    "\n",
    "# Read the new OMX trip table in the disruption with resilience case\n",
    "matrix_newdisruptresil_matrix_filename = os.path.join(matrix_omx_folder_path, 'new_demand_summed.omx')\n",
    "matrix_newdisruptresil_dem, matrix_resil_trips_matrix_size, matrix_newdisruptresil_trip_omx_file = readOMX(matrix_newdisruptresil_matrix_filename, 'matrix', 0)\n",
    "df_matrix_resil_trips = pd.DataFrame(data=matrix_newdisruptresil_dem)\n",
    "\n",
    "# Read the new OMX trip table in the disruption WITHOUT resilience case\n",
    "matrix_newdisruptNOresil_matrix_filename = os.path.join(matrix_omx_folder_path_noresil, 'new_demand_summed.omx')\n",
    "matrix_newdisruptNOresil_dem, matrix_noresil_trips_matrix_size, matrix_newdisruptNOresil_trip_omx_file = readOMX(matrix_newdisruptNOresil_matrix_filename, 'matrix', 0)\n",
    "df_matrix_NOresil_trips = pd.DataFrame(data=matrix_newdisruptNOresil_dem)\n",
    "\n",
    "# READING THE TABLES FOR \"NOCAR,\" if applicable\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "\n",
    "    # Read the base OMX trip table\n",
    "    nocar_base_matrix_filename = os.path.join(nocar_omx_folder_path_base, 'base_demand_summed.omx')\n",
    "    nocar_base_dem, nocar_base_trips_matrix_size, nocar_base_trip_omx_file = readOMX(nocar_base_matrix_filename, 'nocar', 0)\n",
    "    df_nocar_base_trips = pd.DataFrame(data=nocar_base_dem)\n",
    "\n",
    "    # Read the new OMX trip table in the disruption with resilience case\n",
    "    nocar_newdisruptresil_matrix_filename = os.path.join(nocar_omx_folder_path, 'new_demand_summed.omx')\n",
    "    nocar_newdisruptresil_dem, nocar_resil_trips_matrix_size, nocar_newdisruptresil_trip_omx_file = readOMX(nocar_newdisruptresil_matrix_filename, 'matrix', 0)\n",
    "    df_nocar_resil_trips = pd.DataFrame(data=nocar_newdisruptresil_dem)\n",
    "\n",
    "    # Read the new OMX trip table in the disruption WITHOUT resilience case\n",
    "    nocar_newdisruptNOresil_matrix_filename = os.path.join(nocar_omx_folder_path_noresil, 'new_demand_summed.omx')\n",
    "    nocar_newdisruptNOresil_dem, nocar_noresil_trips_matrix_size, nocar_newdisruptNOresil_trip_omx_file = readOMX(nocar_newdisruptNOresil_matrix_filename, 'matrix', 0)\n",
    "    df_nocar_NOresil_trips = pd.DataFrame(data=nocar_newdisruptNOresil_dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7665e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of file skims\n",
    "baseskims_filename = run_type + '_' + socio + projgroup\n",
    "disruptskims_noresil_filename = run_type + '_disrupt_' + socio + projgroup + '_' + baseline + '_' + elasname + '_' + hazard + '_' + recovery\n",
    "disruptskims_resil_filename = run_type + '_disrupt_' + socio + projgroup + '_' + resil + '_' + elasname + '_' + hazard + '_' + recovery\n",
    "\n",
    "# Create filepath strings for \"matrix\" tables\n",
    "matrix_baseskims_folder = os.path.join(matrix_omx_folder_path_base, baseskims_filename + '.omx')\n",
    "matrix_disruptskims_noresil_folder = os.path.join(matrix_omx_folder_path_noresil, disruptskims_noresil_filename + '.omx')\n",
    "matrix_disruptskims_resil_folder = os.path.join(matrix_omx_folder_path, disruptskims_resil_filename + '.omx')\n",
    "\n",
    "# Create filepath strings for \"nocar\" tables\n",
    "nocar_baseskims_folder = os.path.join(nocar_omx_folder_path_base, baseskims_filename + '.omx')\n",
    "nocar_disruptskims_noresil_folder = os.path.join(nocar_omx_folder_path_noresil, disruptskims_noresil_filename + '.omx')\n",
    "nocar_disruptskims_resil_folder = os.path.join(nocar_omx_folder_path, disruptskims_resil_filename + '.omx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd566457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING THE BASE SKIMS FOR \"MATRIX\"\n",
    "# Read the base skims OMX for \"matrix\"\n",
    "matrix_base_hours, matrix_base_hours_matrix_size, base_skims_omx_file = readOMX(matrix_baseskims_folder, hours_name, 0)\n",
    "df_matrix_base_hours = pd.DataFrame(data=matrix_base_hours)\n",
    "matrix_base_miles, matrix_base_miles_matrix_size, base_skims_omx_file = readOMX(matrix_baseskims_folder, miles_name, 0)\n",
    "df_matrix_base_miles = pd.DataFrame(data=matrix_base_miles)\n",
    "\n",
    "# READING THE BASE SKIMS FOR \"NOCAR,\" if applicable\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    # Read the base skims OMX for \"nocar\"\n",
    "    nocar_base_hours, nocar_base_hours_matrix_size, base_skims_omx_file = readOMX(nocar_baseskims_folder, hours_name, 0)\n",
    "    df_nocar_base_hours = pd.DataFrame(data=nocar_base_hours)\n",
    "    nocar_base_miles, nocar_base_miles_matrix_size, base_skims_omx_file = readOMX(nocar_baseskims_folder, miles_name, 0)\n",
    "    df_nocar_base_miles = pd.DataFrame(data=nocar_base_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING THE DISRUPTION WITH NO RESILIENCE SKIMS FOR \"MATRIX\"\n",
    "# Read the disrupt skims OMX - no resilience project\n",
    "matrix_disrupt_noresil_hours, matrix_size, disrupt_noresil_skims_omx_file = readOMX(matrix_disruptskims_noresil_folder, hours_name, 0)\n",
    "df_matrix_disrupt_noresil_hours = pd.DataFrame(data=matrix_disrupt_noresil_hours)\n",
    "matrix_disrupt_noresil_miles, matrix_size, disrupt_noresil_skims_omx_file = readOMX(matrix_disruptskims_noresil_folder, miles_name, 0)\n",
    "df_matrix_disrupt_noresil_miles = pd.DataFrame(data=matrix_disrupt_noresil_miles)\n",
    "\n",
    "# READING THE DISRUPTION WITH NO RESILIENCE SKIMS FOR \"NOCAR,\" if applicable\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    # Read the disrupt skims OMX - no resilience project\n",
    "    nocar_disrupt_noresil_hours, matrix_size, disrupt_noresil_skims_omx_file = readOMX(nocar_disruptskims_noresil_folder, hours_name, 0)\n",
    "    df_nocar_disrupt_noresil_hours = pd.DataFrame(data=nocar_disrupt_noresil_hours)\n",
    "    nocar_disrupt_noresil_miles, matrix_size, disrupt_noresil_skims_omx_file = readOMX(nocar_disruptskims_noresil_folder, miles_name, 0)\n",
    "    df_nocar_disrupt_noresil_miles = pd.DataFrame(data=nocar_disrupt_noresil_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING THE DISRUPTION WITH RESILIENCE SKIMS FOR \"MATRIX\"\n",
    "# Read the disrupt skims OMX - with resilience project\n",
    "matrix_disrupt_resil_hours, matrix_size, disrupt_resil_skims_omx_file = readOMX(matrix_disruptskims_resil_folder, hours_name, 0)\n",
    "df_matrix_disrupt_resil_hours = pd.DataFrame(data=matrix_disrupt_resil_hours)\n",
    "matrix_disrupt_resil_miles, matrix_size, disrupt_resil_skims_omx_file = readOMX(matrix_disruptskims_resil_folder, miles_name, 0)\n",
    "df_matrix_disrupt_resil_miles = pd.DataFrame(data=matrix_disrupt_resil_miles)\n",
    "\n",
    "# READING THE DISRUPTION WITH RESILIENCE SKIMS FOR \"NOCAR,\" if applicable\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    # Read the disrupt skims OMX - with resilience project\n",
    "    nocar_disrupt_resil_hours, matrix_size, disrupt_resil_skims_omx_file = readOMX(nocar_disruptskims_resil_folder, hours_name, 0)\n",
    "    df_nocar_disrupt_resil_hours = pd.DataFrame(data=nocar_disrupt_resil_hours)\n",
    "    nocar_disrupt_resil_miles, matrix_size, disrupt_resil_skims_omx_file = readOMX(nocar_disruptskims_resil_folder, miles_name, 0)\n",
    "    df_nocar_disrupt_resil_miles = pd.DataFrame(data=nocar_disrupt_resil_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create dataframe based on skim results\n",
    "def makeskimresult_df(hours_df,trips_df,miles_df):\n",
    "    # Base times and distances by origin TAZ\n",
    "    # Convert O-D matrix to tall table indexed by origin and destination TAZ\n",
    "    bool_base_hours = hours_df < largeval\n",
    "    a = np.repeat(bool_base_hours.columns, len(bool_base_hours.index))\n",
    "    b = np.tile(bool_base_hours.index, len(bool_base_hours.columns))\n",
    "\n",
    "    # Sums demand where <largeval\n",
    "    base_cumtripcount = (trips_df.where(bool_base_hours, other=0))\n",
    "    base_cumtime = (base_cumtripcount*hours_df)/60\n",
    "    base_cumdist = (base_cumtripcount*miles_df)\n",
    "    c1 = base_cumtripcount.values.ravel()\n",
    "    c2 = base_cumtime.values.ravel()\n",
    "    c3 = base_cumdist.values.ravel()\n",
    "    df = pd.DataFrame({'from':a, 'to':b, 'trips':c1, 'hours':c2, 'miles':c3})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframes for \"matrix\"\n",
    "matrix_base_df = makeskimresult_df(df_matrix_base_hours,df_matrix_base_trips,df_matrix_base_miles)\n",
    "matrix_disrupt_noresil_df = makeskimresult_df(df_matrix_disrupt_noresil_hours,df_matrix_NOresil_trips,df_matrix_disrupt_noresil_miles)\n",
    "matrix_disrupt_resil_df = makeskimresult_df(df_matrix_disrupt_resil_hours,df_matrix_resil_trips,df_matrix_disrupt_resil_miles)\n",
    "\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    # Make dataframes for \"nocar,\" if applicable\n",
    "    nocar_base_df = makeskimresult_df(df_nocar_base_hours,df_nocar_base_trips,df_nocar_base_miles)\n",
    "    nocar_disrupt_noresil_df = makeskimresult_df(df_nocar_disrupt_noresil_hours,df_nocar_NOresil_trips,df_nocar_disrupt_noresil_miles)\n",
    "    nocar_disrupt_resil_df = makeskimresult_df(df_nocar_disrupt_resil_hours,df_nocar_resil_trips,df_nocar_disrupt_resil_miles)\n",
    "\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    # If \"nocar\" tables exist, combine \"matrix\" and \"nocar\" dataframes for overall depiction of results\n",
    "    base_df = matrix_base_df.add(nocar_base_df)\n",
    "    disrupt_noresil_df = matrix_disrupt_noresil_df.add(nocar_disrupt_noresil_df)\n",
    "    disrupt_resil_df = matrix_disrupt_resil_df.add(nocar_disrupt_resil_df)\n",
    "else:\n",
    "    # Otherwise if \"nocar\" tables do not exist, the overall results are just those from the \"matrix\" folders\n",
    "    base_df = matrix_base_df\n",
    "    disrupt_noresil_df = matrix_disrupt_noresil_df\n",
    "    disrupt_resil_df = matrix_disrupt_resil_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_base_trip_omx_file.close()\n",
    "matrix_newdisruptresil_trip_omx_file.close()\n",
    "matrix_newdisruptNOresil_trip_omx_file.close()\n",
    "\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    nocar_base_trip_omx_file.close()\n",
    "    nocar_newdisruptresil_trip_omx_file.close()\n",
    "    nocar_newdisruptNOresil_trip_omx_file.close()\n",
    "\n",
    "base_skims_omx_file.close()\n",
    "disrupt_noresil_skims_omx_file.close()\n",
    "disrupt_resil_skims_omx_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d824d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of skim results\n",
    "merged_df = pd.merge(base_df, disrupt_noresil_df, how='inner', on=['from', 'to'], suffixes=(\"_base\", None))\n",
    "taz_pair_skims = pd.merge(merged_df, disrupt_resil_df, how='inner', on=['from', 'to'], suffixes=(\"_disrupt_noresil\", \"_disrupt_resil\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 'external'. These are for nodes which do not exist in the TAZ file, and therefore do not have any equity attributes. \n",
    "# They are nodes which are outside the MPO boundaries and are needed for travel demand modeling purposes, but do not have shapes associated with them. \n",
    "# They are not omitted because the totals for hours, miles, and trips should be the same at the MPO level as what is reported to users.\n",
    "taz_pair_skims[['from', 'to']] = taz_pair_skims[['from', 'to']].fillna('external')\n",
    "\n",
    "# Calculate relative change in trips/hours/miles for each \n",
    "taz_pair_skims['trips_delta'] = (taz_pair_skims['trips_disrupt_resil'] - taz_pair_skims['trips_disrupt_noresil'])\n",
    "taz_pair_skims['hours_delta'] = (taz_pair_skims['hours_disrupt_resil'] - taz_pair_skims['hours_disrupt_noresil'])\n",
    "taz_pair_skims['miles_delta'] = (taz_pair_skims['miles_disrupt_resil'] - taz_pair_skims['miles_disrupt_noresil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa057f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three variables to flag whether the disruption is relevant for the TAZ pair (for trips/miles/hours)\n",
    "taz_pair_skims['trips_disruption_relevant'] = taz_pair_skims['trips_base'] != taz_pair_skims['trips_disrupt_noresil']\n",
    "taz_pair_skims['hours_disruption_relevant'] = taz_pair_skims['hours_base'] != taz_pair_skims['hours_disrupt_noresil']\n",
    "taz_pair_skims['miles_disruption_relevant'] = taz_pair_skims['miles_base'] != taz_pair_skims['miles_disrupt_noresil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in equity category label by TAZ\n",
    "taz_equity = pd.read_csv(os.path.join(equity_dir, category_filename + '.csv'),\n",
    "                         usecols=['TAZ', category_name],\n",
    "                         converters={'TAZ': int, category_name: float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate and calculate metrics by TAZ of origin or destination\n",
    "def aggregate(from_or_to):\n",
    "    summary = pd.pivot_table(taz_pair_skims, index=from_or_to, values=taz_pair_skims.columns.to_list(),\n",
    "                                      aggfunc={'trips_base':np.sum,\n",
    "                                               'trips_disrupt_noresil':np.sum,\n",
    "                                               'trips_disrupt_resil': np.sum,\n",
    "                                               'hours_base':np.sum,\n",
    "                                               'hours_disrupt_noresil':np.sum,\n",
    "                                               'hours_disrupt_resil': np.sum,\n",
    "                                               'miles_base':np.sum,\n",
    "                                               'miles_disrupt_noresil':np.sum,\n",
    "                                               'miles_disrupt_resil': np.sum,\n",
    "                                               }, \n",
    "                                               fill_value=0)\n",
    "    summary = pd.DataFrame(summary.to_records())\n",
    "\n",
    "    # MINUTES PER TRIP calculations\n",
    "    summary['minutespertrip_base'] = (summary['hours_base']*60)/summary['trips_base']\n",
    "    summary['minutespertrip_disrupt_noresil'] = (summary['hours_disrupt_noresil']*60)/summary['trips_disrupt_noresil']\n",
    "    summary['minutespertrip_disrupt_resil'] = (summary['hours_disrupt_resil']*60)/summary['trips_disrupt_resil']\n",
    "    # MILES PER TRIP calculations\n",
    "    summary['milespertrip_base'] = (summary['miles_base'])/summary['trips_base']\n",
    "    summary['milespertrip_disrupt_noresil'] = (summary['miles_disrupt_noresil'])/summary['trips_disrupt_noresil']\n",
    "    summary['milespertrip_disrupt_resil'] = (summary['miles_disrupt_resil'])/summary['trips_disrupt_resil']    \n",
    "    # Additional trip calculations\n",
    "    metric = \"trips\"\n",
    "    summary[metric+'_percent_change_noresil'] = ((summary[metric+'_disrupt_noresil'] - summary[metric+'_base'])*100)/summary[metric+'_base']\n",
    "    summary[metric+\"_delta_absolute\"] = summary[metric+'_disrupt_resil'] - summary[metric+'_disrupt_noresil']\n",
    "    summary[metric+'_delta_relative'] = (summary[metric+'_delta_absolute']*100)/summary[metric+'_disrupt_noresil']\n",
    "    # Additional minutes per trip calculations\n",
    "    metric = \"minutespertrip\"\n",
    "    summary[metric+'_percent_change_noresil'] = ((summary[metric+'_disrupt_noresil'] - summary[metric+'_base'])*100)/summary[metric+'_base']\n",
    "    summary[metric+\"_delta_absolute\"] = summary[metric+'_disrupt_resil'] - summary[metric+'_disrupt_noresil']\n",
    "    summary[metric+'_delta_relative'] = (summary[metric+'_delta_absolute']*100)/summary[metric+'_disrupt_noresil']\n",
    "    # Additional miles per trip calculations\n",
    "    metric = \"milespertrip\"\n",
    "    summary[metric+'_percent_change_noresil'] = ((summary[metric+'_disrupt_noresil'] - summary[metric+'_base'])*100)/summary[metric+'_base']\n",
    "    summary[metric+\"_delta_absolute\"] = summary[metric+'_disrupt_resil'] - summary[metric+'_disrupt_noresil']\n",
    "    summary[metric+'_delta_relative'] = (summary[metric+'_delta_absolute']*100)/summary[metric+'_disrupt_noresil']    \n",
    "\n",
    "    # Join by 'from' TAZ or 'to' TAZ (as the case may be)\n",
    "    summary = summary.merge(taz_equity, how='left', left_on=from_or_to, right_on='TAZ')\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAZ_origin_stats = aggregate('from')\n",
    "TAZ_destination_stats = aggregate('to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0180b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_csv_summary_filepath = os.path.join(equity_dir,\"MetricsByTAZ_summary_{}_byTAZofOrigin.csv\".format(equity_cfg['run_id']))\n",
    "# Produce a summary CSV file\n",
    "TAZ_origin_stats.to_csv(origin_csv_summary_filepath)\n",
    "\n",
    "destination_csv_summary_filepath = os.path.join(equity_dir,\"MetricsByTAZ_summary_{}_byTAZofDestination.csv\".format(equity_cfg['run_id']))\n",
    "# Produce a summary CSV file\n",
    "TAZ_destination_stats.to_csv(destination_csv_summary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91338912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prerequisite dictionaries and list to use the \"make_plots\" function and the \"runstats\" function\n",
    "ylabel_dict = {\"percent_change_noresil\":\"Percent Change in \",\n",
    "\"delta_absolute\":\"Change in \",\n",
    "\"delta_relative\":\"Percent Change in \",\n",
    "\"base\":\"\"}\n",
    "\n",
    "title_dict = {\"percent_change_noresil\":\"Percent Change from Baseline Due to Disruption (without Resilience) (i.e., Computing Difference in Metric Compared to Base Value, Then Dividing by Base Value)\",\n",
    "\"delta_absolute\":\"Overall Impact of Resilience Investment as Compared to 'No Resilience' Case, for All TAZ (i.e., Metric in 'Resilience' Case Minus Metric in 'No Resilience' Case)\",\n",
    "\"delta_relative\":\"Relative Impact of Resilience Investment as Compared to 'No Resilience' Case, for All TAZ (i.e., Overall Impact Divided by Value of Metric in 'No Resilience' Case)\",\n",
    "\"base\":\"Baseline Magnitude of Metrics Absent Disruption\"}\n",
    "\n",
    "color_dict = {\"percent_change_noresil\":'#024a70',\n",
    "\"delta_absolute\":\"#990000\",\n",
    "\"delta_relative\":'#833C0C',\n",
    "\"base\":\"#548235\"}\n",
    "\n",
    "metrics_list = [\"base\",\n",
    "\"percent_change_noresil\",\n",
    "\"delta_absolute\",\n",
    "\"delta_relative\"]\n",
    "\n",
    "metricsubtype = ['trips_','minutespertrip_','milespertrip_']\n",
    "\n",
    "y_hoverformat_dict = {\"percent_change_noresil\":\"%{y:.3}%\",\n",
    "\"delta_absolute\":\"%{y:,.5}\",\n",
    "\"delta_relative\":\"%{y:.3}%\",\n",
    "\"base\":\"%{y:,.7}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44490bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runstats(metricsubtype=metricsubtype,metrics_list=metrics_list):\n",
    "    dict = {}\n",
    "    for type in metricsubtype:\n",
    "        for metric in metrics_list:\n",
    "            baseregress_set = TAZ_origin_stats.filter([category_name,type+metric]).dropna()\n",
    "            result = linregress(baseregress_set[category_name],baseregress_set[type+metric])\n",
    "            dict[type+metric] = [*result]\n",
    "    df = pd.DataFrame.from_dict(dict, orient='index',columns=['slope', 'intercept', 'r_value','p_value','stderr'])\n",
    "    # p-value is derived from the configuration file. The default is 0.05\n",
    "    # Check whether the pvalue that resulted from the chi square test is less than the p-value in the configuration file\n",
    "    df['stat_significance'] = df['p_value'] < pval\n",
    "    df['r_squared'] = df['r_value']**2\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns = {'index':'dependent_variable'})\n",
    "    roworder = ['trips_base', 'minutespertrip_base','milespertrip_base', \n",
    "                'trips_percent_change_noresil', 'minutespertrip_percent_change_noresil','milespertrip_percent_change_noresil',\n",
    "                'trips_delta_absolute','minutespertrip_delta_absolute','milespertrip_delta_absolute',\n",
    "                'trips_delta_relative', 'minutespertrip_delta_relative', 'milespertrip_delta_relative'\n",
    "               ]\n",
    "    # convert first column to 'Categorical' data type with custom order\n",
    "    df['dependent_variable'] = pd.Categorical(df['dependent_variable'], categories=roworder, ordered=True)\n",
    "    # sort the dataframe by 'continent' column\n",
    "    df = df.sort_values(by='dependent_variable')\n",
    "    df = df.filter(['dependent_variable','stat_significance','p_value',\n",
    "                                            'slope', 'intercept', 'r_value','r_squared','stderr'],\n",
    "                                       axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28505c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate plots\n",
    "def make_plots(dataframe,variable_name):\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=('Trips',  'Minutes per Trip', 'Miles per Trip'))\n",
    "    fig.add_trace(go.Scatter( \n",
    "        x=dataframe[category_name], \n",
    "        y=dataframe[\"trips_\"+variable_name],\n",
    "        mode='markers',\n",
    "        marker=dict(color=color_dict[variable_name],opacity=0.3),\n",
    "        text=dataframe['TAZ'],\n",
    "        hovertemplate=\n",
    "        \"<b>TAZ ID: %{text}</b><br>\" +\n",
    "        \"Equity Category of Origin TAZ (\"+category_name+\")\"+\": %{x:.0f}<br>\" +\n",
    "        ylabel_dict[variable_name]+\"Trips: \"+y_hoverformat_dict[variable_name]+\"<br>\" +\n",
    "        #\"TAZ: %{marker.size:,}\" +\n",
    "        \"<extra></extra>\"),\n",
    "        row=1,col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dataframe[category_name], \n",
    "        y=dataframe[\"minutespertrip_\"+variable_name],\n",
    "        mode='markers',\n",
    "        marker=dict(color=color_dict[variable_name],opacity=0.3),\n",
    "        text=dataframe['TAZ'],\n",
    "        hovertemplate=\n",
    "        \"<b>TAZ ID: %{text}</b><br>\" +\n",
    "        \"Equity Category of Origin TAZ (\"+category_name+\")\"+\": %{x:.0f}<br>\" +\n",
    "        ylabel_dict[variable_name]+\"Minutes per Trip: \"+y_hoverformat_dict[variable_name]+\"<br>\" +\n",
    "        #\"TAZ: %{marker.size:,}\" +\n",
    "        \"<extra></extra>\"),\n",
    "        row=1,col=2)\n",
    "    fig.add_trace(go.Scatter( \n",
    "        x=dataframe[category_name], \n",
    "        y=dataframe[\"milespertrip_\"+variable_name],\n",
    "        mode='markers',\n",
    "        marker=dict(color=color_dict[variable_name],opacity=0.3),\n",
    "        text=dataframe['TAZ'],\n",
    "        hovertemplate=\n",
    "        \"<b>TAZ ID: %{text}</b><br>\" +\n",
    "        \"Equity Category of Origin TAZ (\"+category_name+\")\"+\": %{x:.0f}<br>\" +\n",
    "        ylabel_dict[variable_name]+\"Miles per Trip: \"+y_hoverformat_dict[variable_name]+\"<br>\" +\n",
    "        #\"TAZ: %{marker.size:,}\" +\n",
    "        \"<extra></extra>\"),\n",
    "        row=1,col=3)\n",
    "    # edit axis labels\n",
    "    fig['layout']['xaxis']['title']=\"Equity Category of Origin TAZ (\"+category_name+\")\"\n",
    "    fig['layout']['xaxis2']['title']=\"Equity Category of Origin TAZ (\"+category_name+\")\"\n",
    "    fig['layout']['xaxis3']['title']=\"Equity Category of Origin TAZ (\"+category_name+\")\"    \n",
    "    fig['layout']['yaxis']['title']=ylabel_dict[variable_name]+\"Trips\"\n",
    "    fig['layout']['yaxis2']['title']=ylabel_dict[variable_name]+\"Minutes per Trip\"\n",
    "    fig['layout']['yaxis3']['title']=ylabel_dict[variable_name]+\"Miles per Trip\"\n",
    "    fig.update_layout(showlegend=False, title_text=title_dict[variable_name])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20f11c",
   "metadata": {},
   "source": [
    "## Questions and corresponding variables\n",
    "### Question 1: What is the baseline magnitude of the metric?\n",
    "- Variables 1T/1H/1M: Overall magnitude of metric absent disruption\n",
    "    - `trips_base`\n",
    "    - `minutespertrip_base`\n",
    "    - `milespertrip_base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dba6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate baseline plots based on TAZ of origin (question 1)\n",
    "make_plots(TAZ_origin_stats, metrics_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b9e10",
   "metadata": {},
   "source": [
    "### Question 2: How relevant is the disruption?\n",
    "- Variables 2aT/2aH/2aM: Percent change from baseline metric due to disruption (without resilience investment)\n",
    "    - `trips_percent_change_noresil`\n",
    "    - `minutespertrip_percent_change_noresil`\n",
    "    - `milespertrip_percent_change_noresil` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343865b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate relative change plots based on TAZ of origin (question 2)\n",
    "make_plots(TAZ_origin_stats, metrics_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd3a6e",
   "metadata": {},
   "source": [
    "### Question 3: What is the projected impact of the resilience investment?\n",
    "##### Question 3A: What was the absolute impact (change in metric)?\n",
    "- Variables 3aT/3aH/3aM: Overall impact of resilience investment on metrics (i.e., magnitude in the \"resilience\" case minus magnitude in the \"no resilience\" case)\n",
    "    - `trips_delta_absolute`\n",
    "    - `minutespertrip_delta_absolute`\n",
    "    - `milespertrip_delta_absolute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate absolute resilience impact plots based on TAZ of origin (question 3a)\n",
    "make_plots(TAZ_origin_stats, metrics_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ba6d0",
   "metadata": {},
   "source": [
    "##### Question 3B: What was the relative impact (change in metric expressed as a percentage of the \"no resilience\" magnitude)?\n",
    "- Variables 3bT/3bH/3bM: Same as the above set, except divided by the metric in the \"no resilience\" case and multiplied by 100 to show percent change relative to \"no resilience\" case\n",
    "    - `trips_delta_relative`\n",
    "    - `minutespertrip_delta_relative`\n",
    "    - `milespertrip_delta_relative`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97070259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate relative resilience impact plots based on TAZ of origin (question 3b)\n",
    "make_plots(TAZ_origin_stats, metrics_list[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b8ef2f",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "## Context\n",
    "The charts above show points for each TAZ, and show the value of the metric in question (for trips originating from that TAZ) in relation to the equity indicator value for that TAZ. Looking at the collection of all points, one can visually start to infer possible relationships between each metric and the equity indicator value. If there is truly a relationship, there may be implications for equity. To quantify the strength of the relationship and the power of the equity indicator value as a predictor of the metric, we can use a linear regression analysis to find the line of best fit, the r-value and r-squared value, and also use p-value to assess the probability that there is a relationship (i.e., probability that the slope of the slope of the line of best fit is non-zero). \n",
    "\n",
    "The p-value shows the allowable threshold probability for whether an identified relationship may be due to random chance. For example, the `equity_metric.config` has a p-value of 0.05 by default - a p-value of 0.05 means that there is a 5 percent chance that there actually is no relationship between the variables. Conversely, there is a 95 percent chance that there is truly a relationship and the slope of the line of best fit is non-zero.  The determination of statistical \"significance\" is based on whether the p-value that resulted from the test is lower than the p-value supplied by the user. If the p-value in the `equity_metric.config` is 0.05 and the test resulted in a p-value of 0.02, then the result was statistically significant.\n",
    "\n",
    "The r-value (correlation) measures the strength of the association between two variables. It ranges from -1 (strongly negatively correlated) to 1 (strongly positively correlated). A value of 0 means there is no correlation. Squaring the r-value results in r-squared, which is a related but different metric that indicates the extent to which the variation in the metric is explained by the modeled relationship with the equity indicator variable. R-squared ranges from 0 to 1. R-squared of 0 means the points are not explained by the regression. R-squared of 1 means that all the points are explained by the regression line. The threshold range for an r-squared value that would indicate a useful or meaningful predictive power depends on the context, but in general lower values indicate that the model is less meaningful and higher values indicate that it is more meaningful.\n",
    "\n",
    "## Results Specific to this Analysis\n",
    "In the table below, a number with \"e\" represents scientific notation. For example, 2.29e-02 means 2.29x10^2 or 0.0229. \n",
    "As a reminder, these are the meanings of the metrics (in the first column of the table):\n",
    "- Overall magnitude of metric absent disruption\n",
    "    - `trips_base`\n",
    "    - `minutespertrip_base`\n",
    "    - `milespertrip_base`\n",
    "- Percent change from baseline metric due to disruption (without resilience investment)\n",
    "    - `trips_percent_change_noresil`\n",
    "    - `minutespertrip_percent_change_noresil`\n",
    "    - `milespertrip_percent_change_noresil` \n",
    "- Overall impact of resilience investment on metrics (i.e., magnitude in the \"resilience\" case minus magnitude in the \"no resilience\" case)\n",
    "    - `trips_delta_absolute`\n",
    "    - `minutespertrip_delta_absolute`\n",
    "    - `milespertrip_delta_absolute`\n",
    "- Same as the above set, except divided by the metric in the \"no resilience\" case and multiplied by 100 to show percent change relative to \"no resilience\" case\n",
    "    - `trips_delta_relative`\n",
    "    - `minutespertrip_delta_relative`\n",
    "    - `milespertrip_delta_relative`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressiontable = runstats(metricsubtype, metrics_list)\n",
    "regressiontable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a090d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots based on TAZ of origin\n",
    "#for metric in metrics_list:\n",
    "#    make_plots(TAZ_origin_stats, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to HTML has moved to TAZ_metrics.py\n",
    "# !jupyter nbconvert MetricsByTAZ.ipynb --to html --no-input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
