{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6059d699",
   "metadata": {},
   "source": [
    "# Equity Analysis at the TAZ Level\n",
    "This Jupyter notebook takes AequilibraE runs (with and without resilience investment) and outputs an HTML file that reports changes in metrics by equity category. \n",
    "\n",
    "The default assumption is that the user will run the equity overlay analysis (`run_equity_overlay.bat` file in C:\\GitHub\\RDR\\helper_tools\\equity_analysis) as a first step, and then use the output from that as an input to this TAZ metrics analysis. However, the user may also directly provide data in a CSV file assigning an equity variable value to each TAZ from another source, rather than running the equity overlay analysis. If providing other data, the equity data must be numeric and either binary or ordinal – data in some other form will produce unexpected results. Future versions of this tool may be made flexible to accept more diverse types of equity data. In order to present a distilled summary of results by equity category, the tool assumes that the data are ordinal and therefore assigns an equity category to each TAZ pair based on the equity category value for either the origin TAZ or the destination TAZ (whichever is higher). It then groups all TAZ pairs by equity category and summarizes respective results for each category. Consult the RDR User Guide in C:\\GitHub\\RDR\\documentation for more information on how to use and understand this tool.\n",
    "\n",
    "The purpose is to help the user examine and understand differential impacts of (A) a resilience investment intended to mitigate effects of (B) a disruption, comparing different equity categories of interest. The analysis displays variables to help illuminate the following questions from various angles.\n",
    "\n",
    "Questions driving this analysis include:\n",
    "- What is the baseline magnitude of trips/hours/miles for each equity category?\n",
    "- How relevant is the disruption for each equity category?\n",
    "- What is the projected impact of the resilience investment overall and for each equity category, i.e., are the benefits equitably distributed?\n",
    "\n",
    "The `equity_metrics.config` configuration file allows the user to specify the following:\n",
    "- `path_to_RDR_config_file` – This should identify the location of the configuration file pertinent to the existing RDR Metamodel run and corresponding AequilibraE runs that will be used for this equity analysis. The analysis will use this configuration file to identify where to access the OMX files from those runs.\n",
    "- `resil` - Resilience project.\n",
    "- `hazard`, `recovery`, `socio`, `proj_group`, `elasticity`, `run_type` - Aequilibrae scenario dimensions.\n",
    "- *Note*: As described above, the default assumption is that the user will use the equity overlay analysis first, and then use the output from that as an input to this TAZ metrics analysis. If the user will instead directly provide the equity data then the user should update the `equity_metrics.config` file to specify the name of the user-provided file in `output_name` (without the CSV file extension). The equity data must be numeric and either binary or ordinal.\n",
    "\n",
    "## Questions and corresponding variables\n",
    "### Question 1: What is the baseline magnitude of trips/hours/miles for each equity category?\n",
    "- Variables 1T/1H/1M: Overall sum of trips/hours/miles for each category absent disruption\n",
    "    - `trips_base`\n",
    "    - `hours_base`\n",
    "    - `miles_base`\n",
    "    \n",
    "### Question 2: How relevant is the disruption for each equity category?\n",
    "- Variables 2aT/2aH/2aM: Percent change from baseline trips/hours/miles due to disruption (without resilience investment)\n",
    "    - `trips_percent_change_noresil`\n",
    "    - `hours_percent_change_noresil`\n",
    "    - `miles_percent_change_noresil` \n",
    "- Variables 2bT/2bH/2bM: Percent of TAZ pairs with a change in trips/hours/miles due to disruption (without resilience investment)\n",
    "    - `trips_percent_pairs_relevant`\n",
    "    - `hours_percent_pairs_relevant`\n",
    "    - `miles_percent_pairs_relevant`\n",
    "    \n",
    "### Question 3: What is the projected impact of the resilience investment for each equity category?\n",
    "- Variables 3aT/3aH/3aM: Overall impact of resilience investment in trips/hours/miles (i.e., trips/hours/miles in the \"resilience\" case minus trips/hours/miles in the \"no resilience\" case)\n",
    "    - `trips_delta_absolute`\n",
    "    - `hours_delta_absolute`\n",
    "    - `miles_delta_absolute`\n",
    "- Variables 3bT/3bH/3bM: Same as the above set, except divided by the trips/hours/miles in the \"no resilience\" case and multiplied by 100 to show percent change relative to \"no resilience\" case\n",
    "    - `trips_delta_relative`\n",
    "    - `hours_delta_relative`\n",
    "    - `miles_delta_relative`\n",
    "- Variables 3cT/3cH/3cM: Average difference in trips/hours/miles due to resilience investment for all __relevant__ TAZ pairs (i.e., among the subset of TAZ pairs where there was a disruption impact in the \"no resilience\" case)\n",
    "    - `trips_mean_delta_for_relevant_pairs`\n",
    "    - `hours_mean_delta_for_relevant_pairs`\n",
    "    - `miles_mean_delta_for_relevant_pairs`\n",
    "- Variables 3dT/3dH/3dM: Average difference in trips/hours/miles due to resilience investment for all TAZ pairs __with non-zero delta due to resilience__ (i.e., among the even smaller subset of TAZ pairs where the \"resilience\" case was different from the \"no resilience\" case)\n",
    "    - `trips_mean_delta_for_pairs_with_non-zero_delta`\n",
    "    - `hours_mean_delta_for_pairs_with_non-zero_delta`\n",
    "    - `miles_mean_delta_for_pairs_with_non-zero_delta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openmatrix as omx\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import equity_config_reader\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../../metamodel_py'))\n",
    "import rdr_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The equity configuration filepath is now passed into this notebook from its parent, TAZ_metrics.py, via the temporary text file,\n",
    "# assuming this notebook and TAZ_metrics.py are both in the same folder.\n",
    "with open('temp.txt', 'r') as f:\n",
    "    config_filepath = f.read()\n",
    "equity_cfg = equity_config_reader.read_equity_config_file(config_filepath)\n",
    "\n",
    "rdr_cfg_path = equity_cfg['path_to_RDR_config_file']\n",
    "\n",
    "# Directory of equity helper tool files\n",
    "equity_dir = equity_cfg['equity_analysis_dir']\n",
    "\n",
    "cfg = rdr_setup.read_config_file(rdr_cfg_path)\n",
    "\n",
    "RDR_run_id = cfg['run_id']\n",
    "\n",
    "# Name of equity variable\n",
    "category_name = equity_cfg['equity_feature']\n",
    "\n",
    "# Name of CSV file with equity category value for each TAZ (either output from run_equity_overlay.bat OR user-provided)\n",
    "category_filename = equity_cfg['output_name']\n",
    "\n",
    "# Look to see if the equity overlay data exists\n",
    "if not os.path.exists(os.path.join(equity_dir, category_filename + '.csv')):\n",
    "    print('{}.csv not found in {}. Please run the equity_overlay first or generate your own file and specify the filename for it as output_name in the equity_metrics.config file'.format(category_filename, equity_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c964bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility method for reading OMX files\n",
    "def readOMX(filename, selectedMatrix, debug_mode):\n",
    "    f = omx.open_file(filename)\n",
    "    matrix_size = f.shape()\n",
    "    if debug_mode:\n",
    "        print('Shape: ', f.shape())\n",
    "        print('Number of tables: ', len(f))\n",
    "        print('Table names: ', f.list_matrices())\n",
    "        print('Attributes: ', f.list_all_attributes())\n",
    "    omx_df = f[selectedMatrix]\n",
    "    if debug_mode:\n",
    "        print('Sum of matrix elements: ', '{:.9}'.format(np.sum(omx_df)))\n",
    "        print('Percentiles: ', np.percentile(omx_df, (1, 10, 30, 50, 70, 90, 99)))\n",
    "        print('Maximum: ', np.amax(omx_df))\n",
    "    return omx_df, matrix_size, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for comparison - these come from equity_cfg, the equity_metrics.config file\n",
    "resil = equity_cfg['resil']\n",
    "baseline = equity_cfg['baseline']\n",
    "hazard = equity_cfg['hazard']\n",
    "recovery = equity_cfg['recovery']\n",
    "socio = equity_cfg['socio']\n",
    "projgroup = equity_cfg['projgroup']\n",
    "elasticity = equity_cfg['elasticity']\n",
    "elasname = str(int(10 * -elasticity))\n",
    "run_type = equity_cfg['run_type']\n",
    "largeval = int(equity_cfg['largeval'])\n",
    "\n",
    "if run_type == 'SP':\n",
    "    hours_name = 'free_flow_time'\n",
    "    miles_name = 'distance'\n",
    "else:\n",
    "    hours_name = 'time_final'\n",
    "    miles_name = 'distance_blended'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the OMX files for \"base\" and for \"disruption with resilience investment\"\n",
    "omx_file_path = os.path.join(equity_dir, \"aeq_runs\", \"disrupt\", RDR_run_id,\n",
    "                             socio + projgroup + '_' + resil + '_' + elasname + '_' + hazard + '_' + recovery,\n",
    "                             \"matrix\", \"matrices\")\n",
    "\n",
    "# Location of the OMX files for \"disruption WITHOUT resilience investment\"\n",
    "omx_file_path_noresil = os.path.join(equity_dir, \"aeq_runs\", \"disrupt\", RDR_run_id,\n",
    "                                     socio + projgroup + '_' + baseline + '_' + elasname + '_' + hazard + '_' + recovery,\n",
    "                                     \"matrix\", \"matrices\")\n",
    "\n",
    "# Read the base OMX trip table\n",
    "base_matrix_filename = os.path.join(omx_file_path, 'base_demand_summed.omx')\n",
    "base_dem, matrix_size, base_trip_omx_file = readOMX(base_matrix_filename, 'matrix', 0)\n",
    "df_base_trips = pd.DataFrame(data=base_dem)\n",
    "\n",
    "# Read the new OMX trip table in the disruption with resilience case\n",
    "newdisruptresil_matrix_filename = os.path.join(omx_file_path, 'new_demand_summed.omx')\n",
    "newdisruptresil_dem, matrix_size, newdisruptresil_trip_omx_file = readOMX(newdisruptresil_matrix_filename, 'matrix', 0)\n",
    "df_resil_trips = pd.DataFrame(data=newdisruptresil_dem)\n",
    "\n",
    "# Read the new OMX trip table in the disruption WITHOUT resilience case\n",
    "newdisruptNOresil_matrix_filename = os.path.join(omx_file_path_noresil, 'new_demand_summed.omx')\n",
    "newdisruptNOresil_dem, matrix_size, newdisruptNOresil_trip_omx_file = readOMX(newdisruptNOresil_matrix_filename, 'matrix', 0)\n",
    "df_NOresil_trips = pd.DataFrame(data=newdisruptNOresil_dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7665e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filename strings\n",
    "baseskims_filename = run_type + '_' + socio + projgroup\n",
    "baseskims_folder = os.path.join(omx_file_path, baseskims_filename + '.omx')\n",
    "disruptskims_noresil_filename = run_type + '_disrupt_' + socio + projgroup + '_' + baseline + '_' + elasname + '_' + hazard + '_' + recovery\n",
    "disruptskims_noresil_folder = os.path.join(omx_file_path_noresil, disruptskims_noresil_filename + '.omx')\n",
    "disruptskims_resil_filename = run_type + '_disrupt_' + socio + projgroup + '_' + resil + '_' + elasname + '_' + hazard + '_' + recovery\n",
    "disruptskims_resil_folder = os.path.join(omx_file_path, disruptskims_resil_filename + '.omx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd566457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the base skims OMX\n",
    "base_hours, matrix_size, base_skims_omx_file = readOMX(baseskims_folder, hours_name, 0)\n",
    "df_base_hours = pd.DataFrame(data=base_hours)\n",
    "base_miles, matrix_size, base_skims_omx_file = readOMX(baseskims_folder, miles_name, 0)\n",
    "df_base_miles = pd.DataFrame(data=base_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base times and distances by origin TAZ\n",
    "# Convert O-D matrix to tall table indexed by origin and destination TAZ\n",
    "bool_base_hours = df_base_hours < largeval\n",
    "a = np.repeat(bool_base_hours.columns, len(bool_base_hours.index))\n",
    "b = np.tile(bool_base_hours.index, len(bool_base_hours.columns))\n",
    "\n",
    "# Sums demand where <largeval\n",
    "base_cumtripcount = (df_base_trips.where(bool_base_hours, other=0))\n",
    "base_cumtime = (base_cumtripcount*df_base_hours)/60\n",
    "base_cumdist = (base_cumtripcount*df_base_miles)\n",
    "c1 = base_cumtripcount.values.ravel()\n",
    "c2 = base_cumtime.values.ravel()\n",
    "c3 = base_cumdist.values.ravel()\n",
    "base_df = pd.DataFrame({'from':a, 'to':b, 'trips':c1, 'hours':c2, 'miles':c3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the disrupt skims OMX - no resilience project\n",
    "disrupt_noresil_hours, matrix_size, disrupt_noresil_skims_omx_file = readOMX(disruptskims_noresil_folder, hours_name, 0)\n",
    "df_disrupt_noresil_hours = pd.DataFrame(data=disrupt_noresil_hours)\n",
    "disrupt_noresil_miles, matrix_size, disrupt_noresil_skims_omx_file = readOMX(disruptskims_noresil_folder, miles_name, 0)\n",
    "df_disrupt_noresil_miles = pd.DataFrame(data=disrupt_noresil_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disrupt times and distances for no resilience project by origin TAZ\n",
    "# Convert O-D matrix to tall table indexed by origin and destination TAZ\n",
    "bool_disrupt_noresil_hours = df_disrupt_noresil_hours < largeval\n",
    "a = np.repeat(bool_disrupt_noresil_hours.columns, len(bool_disrupt_noresil_hours.index))\n",
    "b = np.tile(bool_disrupt_noresil_hours.index, len(bool_disrupt_noresil_hours.columns))\n",
    "\n",
    "# Sums demand where <largeval\n",
    "disrupt_noresil_cumtripcount = (df_NOresil_trips.where(bool_disrupt_noresil_hours, other=0))\n",
    "disrupt_noresil_cumtime = (disrupt_noresil_cumtripcount*df_disrupt_noresil_hours)/60\n",
    "disrupt_noresil_cumdist = (disrupt_noresil_cumtripcount*df_disrupt_noresil_miles)\n",
    "c1 = disrupt_noresil_cumtripcount.values.ravel()\n",
    "c2 = disrupt_noresil_cumtime.values.ravel()\n",
    "c3 = disrupt_noresil_cumdist.values.ravel()\n",
    "disrupt_noresil_df = pd.DataFrame({'from':a, 'to':b, 'trips':c1, 'hours':c2, 'miles':c3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the disrupt skims OMX - with resilience project\n",
    "disrupt_resil_hours, matrix_size, disrupt_resil_skims_omx_file = readOMX(disruptskims_resil_folder, hours_name, 0)\n",
    "df_disrupt_resil_hours = pd.DataFrame(data=disrupt_resil_hours)\n",
    "disrupt_resil_miles, matrix_size, disrupt_resil_skims_omx_file = readOMX(disruptskims_resil_folder, miles_name, 0)\n",
    "df_disrupt_resil_miles = pd.DataFrame(data=disrupt_resil_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disrupt times and distances for resilience project by origin TAZ\n",
    "# Convert O-D matrix to tall table indexed by origin and destination TAZ\n",
    "bool_disrupt_resil_hours = df_disrupt_resil_hours < largeval\n",
    "a = np.repeat(bool_disrupt_resil_hours.columns, len(bool_disrupt_resil_hours.index))\n",
    "b = np.tile(bool_disrupt_resil_hours.index, len(bool_disrupt_resil_hours.columns))\n",
    "\n",
    "# Sums demand where <largeval\n",
    "disrupt_resil_cumtripcount = (df_resil_trips.where(bool_disrupt_resil_hours, other=0))\n",
    "disrupt_resil_cumtime = (disrupt_resil_cumtripcount*df_disrupt_resil_hours)/60\n",
    "disrupt_resil_cumdist = (disrupt_resil_cumtripcount*df_disrupt_resil_miles)\n",
    "c1 = disrupt_resil_cumtripcount.values.ravel()\n",
    "c2 = disrupt_resil_cumtime.values.ravel()\n",
    "c3 = disrupt_resil_cumdist.values.ravel()\n",
    "disrupt_resil_df = pd.DataFrame({'from':a, 'to':b, 'trips':c1, 'hours':c2, 'miles':c3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d824d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of skim results\n",
    "merged_df = pd.merge(base_df, disrupt_noresil_df, how='inner', on=['from', 'to'], suffixes=(\"_base\", None))\n",
    "taz_pair_skims = pd.merge(merged_df, disrupt_resil_df, how='inner', on=['from', 'to'], suffixes=(\"_disrupt_noresil\", \"_disrupt_resil\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in equity category label by TAZ\n",
    "taz_equity = pd.read_csv(os.path.join(equity_dir, category_filename + '.csv'),\n",
    "                         usecols=['TAZ', category_name],\n",
    "                         converters={'TAZ': int, category_name: float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912946fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join by from TAZ and to TAZ\n",
    "taz_stats = taz_pair_skims.merge(taz_equity, how='left', left_on='from', right_on='TAZ').merge(taz_equity, how='left', left_on='to', right_on='TAZ', suffixes=('_from', '_to'))\n",
    "\n",
    "# Add label to show the highest value for equity indicator for each TAZ pair, regardless of whether that value is for the origin or destination.\n",
    "# Assign the equity indicator value from the origin or destination (whichever is higher).\n",
    "# If both origin and destination have a value that is not a number (nan), assign 0. For example, \n",
    "# this would be true if the origin and destination are both nodes that do not exist in the TAZ file (external).\n",
    "# This approach relies on the user providing equity data that are numeric, and it assumes that the data are ordinal.\n",
    "conditiona = (pd.isna(taz_stats[category_name + '_from'])) & (pd.isna(taz_stats[category_name + '_to']))\n",
    "conditionb = (pd.isna(taz_stats[category_name + '_to']))\n",
    "conditionc = (pd.isna(taz_stats[category_name + '_from']))\n",
    "conditiond = (taz_stats[category_name + '_from'] >= taz_stats[category_name + '_to'])\n",
    "conditione = (taz_stats[category_name + '_from'] < taz_stats[category_name + '_to'])\n",
    "taz_stats[category_name + '_high_value'] = (\n",
    "    np.select(\n",
    "        condlist=[conditiona,conditionb,conditionc,conditiond,conditione],\n",
    "        choicelist=[float(0),\n",
    "                    taz_stats[category_name + '_from'].astype(float),\n",
    "                    taz_stats[category_name + '_to'].astype(float),\n",
    "                    taz_stats[category_name + '_from'].astype(float),\n",
    "                    taz_stats[category_name + '_to'].astype(float)],\n",
    "        default=0))\n",
    "\n",
    "# Replace NaN values with 'external'. These are for nodes which do not exist in the TAZ file, and therefore do not have any equity attributes. \n",
    "# They are nodes which are outside the MPO boundaries and are needed for travel demand modeling purposes, but do not have shapes associated with them. \n",
    "# They are not omitted because the totals for hours, miles, and trips should be the same at the MPO level as what is reported to users.\n",
    "taz_stats[['TAZ_from', category_name + '_from','TAZ_to', category_name + '_to']] = taz_stats[['TAZ_from', category_name + '_from','TAZ_to', category_name + '_to']].fillna('external')\n",
    "\n",
    "# Add 'O-D_categorylabel' to taz_stats data frame to compile the equity category of the \"to\" and \"from\" TAZ in one attribute\n",
    "taz_stats['O-D_categorylabel'] = taz_stats[category_name + '_from'].astype(str) + \" to \" + taz_stats[category_name + '_to'].astype(str)\n",
    "\n",
    "# Calculate relative change in trips/hours/miles for each \n",
    "taz_stats['trips_delta'] = (taz_stats['trips_disrupt_resil'] - taz_stats['trips_disrupt_noresil'])\n",
    "taz_stats['hours_delta'] = (taz_stats['hours_disrupt_resil'] - taz_stats['hours_disrupt_noresil'])\n",
    "taz_stats['miles_delta'] = (taz_stats['miles_disrupt_resil'] - taz_stats['miles_disrupt_noresil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78cc77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three variables to flag whether the disruption is relevant for the TAZ pair (for trips/miles/hours)\n",
    "taz_stats['trips_disruption_relevant'] = taz_stats['trips_base'] != taz_stats['trips_disrupt_noresil']\n",
    "taz_stats['hours_disruption_relevant'] = taz_stats['hours_base'] != taz_stats['hours_disrupt_noresil']\n",
    "taz_stats['miles_disruption_relevant'] = taz_stats['miles_base'] != taz_stats['miles_disrupt_noresil']\n",
    "\n",
    "# Create three variables to capture the nature of impact for each TAZ pair (for trips/miles/hours), \n",
    "# with and without resilience investment\n",
    "\n",
    "# for trips\n",
    "condition1 = (taz_stats['trips_disrupt_noresil'] == taz_stats['trips_base'])\n",
    "condition2 = (taz_stats['trips_disrupt_resil'] == taz_stats['trips_disrupt_noresil'])\n",
    "condition3 = (taz_stats['trips_disrupt_noresil'] != None)&(taz_stats['trips_disrupt_resil'] != None)\n",
    "taz_stats['trips_delta_category'] = (\n",
    "    np.select(\n",
    "        condlist=[condition1,condition2,condition3], \n",
    "        choicelist=[\"no_change\",\n",
    "                   \"same_change\",\n",
    "                   \"different_change\"], \n",
    "        default=\"null_value\"))\n",
    "\n",
    "# for miles\n",
    "condition1 = (taz_stats['miles_disrupt_noresil'] == taz_stats['miles_base'])\n",
    "condition2 = (taz_stats['miles_disrupt_resil'] == taz_stats['miles_disrupt_noresil'])\n",
    "condition3 = (taz_stats['miles_disrupt_noresil'] != None)&(taz_stats['miles_disrupt_resil'] != None)\n",
    "taz_stats['miles_delta_category'] = (\n",
    "    np.select(\n",
    "        condlist=[condition1,condition2,condition3], \n",
    "        choicelist=[\"no_change\",\n",
    "                   \"same_change\",\n",
    "                   \"different_change\"], \n",
    "        default=\"null_value\"))\n",
    "\n",
    "# for hours\n",
    "condition1 = (taz_stats['hours_disrupt_noresil'] == taz_stats['hours_base'])\n",
    "condition2 = (taz_stats['hours_disrupt_resil'] == taz_stats['hours_disrupt_noresil'])\n",
    "condition3 = (taz_stats['hours_disrupt_noresil'] != None)&(taz_stats['hours_disrupt_resil'] != None)\n",
    "taz_stats['hours_delta_category'] = (\n",
    "    np.select(\n",
    "        condlist=[condition1,condition2,condition3], \n",
    "        choicelist=[\"no_change\",\n",
    "                   \"same_change\",\n",
    "                   \"different_change\"], \n",
    "        default=\"null_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to produce a summary table given a metric of interest (i.e., trips, miles, or hours) \n",
    "# and an index of interest (i.e., some grouping of TAZ pairs based on equity category of origin/destination).\n",
    "\n",
    "# First create helper function for use in the main function\n",
    "def countnonzeros(x):\n",
    "    return x.astype(bool).sum(axis=0)\n",
    "\n",
    "# Now create main function\n",
    "def createsummary(index,metric):\n",
    "    # Aggregate base metric for all three categories (Variables 1T/1H/1M)\n",
    "    category_base = pd.pivot_table(taz_stats, index=index, values=taz_stats.columns.to_list(),\n",
    "                                    aggfunc={metric+'_base':np.sum}, \n",
    "                                    fill_value=0)\n",
    "    # Create variables 2aT/2aH/2aM: Percent change from baseline trips/hours/miles due to disruption (without resilience investment) and merge with the prior into one dataframe\n",
    "    Q_TwoA = pd.pivot_table(taz_stats, index=index, values=taz_stats.columns.to_list(),\n",
    "                                    aggfunc={metric+'_disrupt_noresil':np.sum}, \n",
    "                                    fill_value=0)\n",
    "    category_stats = pd.merge(category_base,Q_TwoA,on=None,left_index=True, right_index=True)\n",
    "    category_stats[metric+'_percent_change_noresil'] = ((category_stats[metric+'_disrupt_noresil'] - category_stats[metric+'_base'])*100)/category_stats[metric+'_base']\n",
    "\n",
    "    # Create variables 2bT/2bH/2bM: Percent of TAZ pairs with a change in trips/hours/miles due to disruption (without resilience investment)\n",
    "    Q_TwoB = pd.pivot_table(taz_stats, index=index, values=taz_stats.columns.to_list(),\n",
    "                                    aggfunc={metric+'_disruption_relevant':[countnonzeros,len]}, \n",
    "                                    fill_value=0)\n",
    "    # Flatten multi index hierarchy in column headers\n",
    "    Q_TwoB = pd.DataFrame(Q_TwoB.to_records())\n",
    "    Q_TwoB[metric+'_percent_pairs_relevant'] = (Q_TwoB[\"('\"+metric+\"_disruption_relevant', 'countnonzeros')\"]*100)/Q_TwoB[\"('\"+metric+\"_disruption_relevant', 'len')\"]\n",
    "    # Merge into category_stats dataframe\n",
    "    category_stats = pd.merge(category_stats,Q_TwoB,on=category_name + '_high_value')\n",
    "\n",
    "    # Create variables 3aT/3aH/3aM: Overall impact of resilience investment in trips/hours/miles (i.e., trips/hours/miles in the \"resilience\" case minus trips/hours/miles in the \"no resilience\" case)\n",
    "    Q_ThreeA = pd.pivot_table(taz_stats, index=index, values=taz_stats.columns.to_list(),\n",
    "                                    aggfunc={metric+'_disrupt_resil':np.sum}, \n",
    "                                    fill_value=0)\n",
    "    category_stats = pd.merge(category_stats,Q_ThreeA,on=category_name + '_high_value')\n",
    "    category_stats[metric+\"_delta_absolute\"] = category_stats[metric+'_disrupt_resil'] - category_stats[metric+'_disrupt_noresil']\n",
    "    # Create variables 3bT/3bH/3bM: Same as the above set, except divided by the trips/hours/miles in the \"no resilience\" case and multiplied by 100 to show percent change relative to \"no resilience\" case\n",
    "    category_stats[metric+'_delta_relative'] = (category_stats[metric+'_delta_absolute']*100)/category_stats[metric+'_disrupt_noresil']\n",
    "\n",
    "    # Create variables 3cT/3cH/3cM: Average difference in trips/hours/miles due to resilience investment for all relevant TAZ pairs (i.e., among the subset of TAZ pairs where there was a disruption impact in the \"no resilience\" case)\n",
    "    # First filter for relevant TAZ pairs\n",
    "    relevant_filter = taz_stats[metric+'_disrupt_noresil'] != taz_stats[metric+'_base'] \n",
    "    relevant_set = taz_stats[relevant_filter]\n",
    "    Q_ThreeC = pd.pivot_table(relevant_set, index=index, values=relevant_set.columns.to_list(),\n",
    "                                    aggfunc={metric+'_delta':np.mean}, \n",
    "                                    fill_value=0)\n",
    "    Q_ThreeC.rename(columns = {metric+'_delta':metric+'_mean_delta_for_relevant_pairs'}, inplace = True)\n",
    "    if metric+'_mean_delta_for_relevant_pairs' in Q_ThreeC.columns.to_list():\n",
    "        category_stats = pd.merge(category_stats,Q_ThreeC,on=category_name + '_high_value')\n",
    "\n",
    "    # Create variables 3dT/3dH/3dM: Average difference in trips/hours/miles due to resilience investment for all TAZ pairs with non-zero delta due to resilience (i.e., among the even smaller subset of TAZ pairs where the \"resilience\" case was different from the \"no resilience\" case)\n",
    "    nonzerodelta_filter = taz_stats[metric+'_disrupt_noresil'] != taz_stats[metric+'_disrupt_resil']\n",
    "    nonzerodelta_set = taz_stats[nonzerodelta_filter]\n",
    "    Q_ThreeD = pd.pivot_table(nonzerodelta_set, index=index, values=nonzerodelta_set.columns.to_list(),\n",
    "                                aggfunc={metric+'_delta':np.mean}, \n",
    "                                fill_value=0)\n",
    "    Q_ThreeD.rename(columns = {metric+'_delta':metric+'_mean_delta_for_pairs_with_non-zero_delta'}, inplace = True)\n",
    "    if metric+'_mean_delta_for_pairs_with_non-zero_delta' in Q_ThreeD.columns.to_list():\n",
    "        category_stats = pd.merge(category_stats,Q_ThreeD,on=category_name + '_high_value')\n",
    "\n",
    "    # Subset the columns of interest\n",
    "    category_stats = category_stats.filter([index,\n",
    "                                            metric+'_base', \n",
    "                                            metric+'_percent_change_noresil', \n",
    "                                            metric+'_percent_pairs_relevant',\n",
    "                                            metric+'_delta_absolute', \n",
    "                                            metric+'_delta_relative',\n",
    "                                            metric+'_mean_delta_for_relevant_pairs', \n",
    "                                            metric+'_mean_delta_for_pairs_with_non-zero_delta'],\n",
    "                                       axis=1)\n",
    "    # Convert the equity category to string for better rendering in the charts that follow.\n",
    "    category_stats[index] = category_stats[index].astype(str)\n",
    "    return category_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0be4c5",
   "metadata": {},
   "source": [
    "### Summary of Indicators for Trips by Equity Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce distilled output table for trips\n",
    "trips_summary_distilled = createsummary(category_name + '_high_value','trips')\n",
    "trips_summary_distilled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db0c14",
   "metadata": {},
   "source": [
    "### Summary of Indicators for Hours by Equity Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab493e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_summary_distilled = createsummary(category_name + '_high_value','hours')\n",
    "hours_summary_distilled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb713a0",
   "metadata": {},
   "source": [
    "### Summary of Indicators for Miles by Equity Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "miles_summary_distilled = createsummary(category_name + '_high_value','miles')\n",
    "miles_summary_distilled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecae82",
   "metadata": {},
   "source": [
    "### Question 1: What is the baseline magnitude of trips/hours/miles for each category?\n",
    "- Variables 1T/1H/1M: Overall sum of trips/hours/miles for each category absent disruption\n",
    "    - `trips_base`\n",
    "    - `hours_base`\n",
    "    - `miles_base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641984b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to generate bar charts for each question of interest\n",
    "def makebarcharts(variabletype,axislabel,title):\n",
    "    if \"trips_\"+variabletype in trips_summary_distilled.columns.to_list():\n",
    "        if (trips_summary_distilled['trips_'+variabletype] == 0).all() and (hours_summary_distilled['hours_'+variabletype] == 0).all() and (miles_summary_distilled['miles_'+variabletype] == 0).all():\n",
    "            print(\"The '___{}' variable was zero for all categories for trips, hours, and miles, so no chart was produced.\".format(variabletype))\n",
    "        else:\n",
    "            fig = make_subplots(rows=1, cols=3)\n",
    "            fig.add_trace(go.Bar(\n",
    "                x= trips_summary_distilled[\"trips_\"+variabletype],\n",
    "                y= trips_summary_distilled[category_name + '_high_value'],\n",
    "                orientation='h'),\n",
    "                row=1, col=1)\n",
    "            fig.add_trace(go.Bar(\n",
    "                x= hours_summary_distilled[\"hours_\"+variabletype],\n",
    "                y= hours_summary_distilled[category_name + '_high_value'],\n",
    "                orientation='h'),\n",
    "                row=1, col=2)\n",
    "            fig.add_trace(go.Bar(\n",
    "                x= miles_summary_distilled[\"miles_\"+variabletype],\n",
    "                y= miles_summary_distilled[category_name + '_high_value'],\n",
    "                orientation='h'),\n",
    "                row=1, col=3)\n",
    "\n",
    "            # Update xaxis properties\n",
    "            fig.update_xaxes(title_text=axislabel+\"Trips\", row=1, col=1)\n",
    "            fig.update_xaxes(title_text=axislabel+\"Hours\", row=1, col=2)\n",
    "            fig.update_xaxes(title_text=axislabel+\"Miles\", row=1, col=3)\n",
    "\n",
    "            # Update yaxis properties\n",
    "            fig.update_yaxes(title_text=\"TAZ Pairs Grouped by their Highest Equity Indicator Value (in Origin or Destination)\", row=1, col=1)\n",
    "\n",
    "            # Update title and height\n",
    "            fig.update_layout(title_text=title, height=700, showlegend=False)\n",
    "\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variabletype = \"base\"\n",
    "axislabel = \"\"\n",
    "title = \"Baseline Magnitude of Trips/Hours/Miles for Each Equity Indicator Category, Absent Disruption\"\n",
    "makebarcharts(variabletype=variabletype,axislabel=axislabel,title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e5a55e",
   "metadata": {},
   "source": [
    "### Question 2: How relevant is the disruption for each category?\n",
    "- Variables 2aT/2aH/2aM: Percent change from baseline trips/hours/miles due to disruption (without resilience investment)\n",
    "    - `trips_percent_change_noresil`\n",
    "    - `hours_percent_change_noresil`\n",
    "    - `miles_percent_change_noresil` \n",
    "- Variables 2bT/2bH/2bM: Percent of TAZ pairs with a change in trips/hours/miles due to disruption (without resilience investment)\n",
    "    - `trips_percent_pairs_relevant`\n",
    "    - `hours_percent_pairs_relevant`\n",
    "    - `miles_percent_pairs_relevant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb091b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "variabletype = \"percent_change_noresil\"\n",
    "axislabel = \"Percent Change in \"\n",
    "title = \"Percent Change from Baseline Due to Disruption (without Resilience)\"\n",
    "makebarcharts(variabletype=variabletype,axislabel=axislabel,title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "variabletype = \"percent_pairs_relevant\"\n",
    "axislabel = \"Percent of Pairs: \"\n",
    "title = \"Percent of TAZ Pairs with Potential Impacts from Disruption\"\n",
    "makebarcharts(variabletype=variabletype,axislabel=axislabel,title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0856d9c",
   "metadata": {},
   "source": [
    "### Question 3: What is the projected impact of the resilience investment for this category?\n",
    "- Variables 3aT/3aH/3aM: Overall impact of resilience investment in trips/hours/miles (i.e., trips/hours/miles in the \"resilience\" case minus trips/hours/miles in the \"no resilience\" case)\n",
    "    - `trips_delta_absolute`\n",
    "    - `hours_delta_absolute`\n",
    "    - `miles_delta_absolute`\n",
    "- Variables 3bT/3bH/3bM: Same as the above set, except divided by the trips/hours/miles in the \"no resilience\" case and multiplied by 100 to show percent change relative to \"no resilience\" case\n",
    "    - `trips_delta_relative`\n",
    "    - `hours_delta_relative`\n",
    "    - `miles_delta_relative`\n",
    "- Variables 3cT/3cH/3cM: Average difference in trips/hours/miles due to resilience investment for all __relevant__ TAZ pairs (i.e., among the subset of TAZ pairs where there was a disruption impact in the \"no resilience\" case)\n",
    "    - `trips_mean_delta_for_relevant_pairs`\n",
    "    - `hours_mean_delta_for_relevant_pairs`\n",
    "    - `miles_mean_delta_for_relevant_pairs`\n",
    "- Variables 3dT/3dH/3dM: Average difference in trips/hours/miles due to resilience investment for all TAZ pairs __with non-zero delta due to resilience__ (i.e., among the even smaller subset of TAZ pairs where the \"resilience\" case was different from the \"no resilience\" case)\n",
    "    - `trips_mean_delta_for_pairs_with_non-zero_delta`\n",
    "    - `hours_mean_delta_for_pairs_with_non-zero_delta`\n",
    "    - `miles_mean_delta_for_pairs_with_non-zero_delta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f783e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "variabletype = \"delta_absolute\"\n",
    "axislabel = \"Change in \"\n",
    "title = 'Overall Impact of Resilience Investment as Compared to the \"No Resilience\" Case, for All TAZ Pairs'\n",
    "makebarcharts(variabletype=variabletype,axislabel=axislabel,title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "variabletype = \"delta_relative\"\n",
    "axislabel = \"Percent Change in \"\n",
    "title = 'Relative Impact of Resilience Investment as Compared to the \"No Resilience\" Case, for All TAZ Pairs'\n",
    "makebarcharts(variabletype=variabletype,axislabel=axislabel,title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f1cbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variabletype = \"mean_delta_for_relevant_pairs\"\n",
    "axislabel = \"Average Change in \"\n",
    "title = 'Average Difference Due to Resilience Investment for Relevant TAZ Pairs (Relevant = Those with Disruption Impact in the \"No Resilience\" Case)'\n",
    "makebarcharts(variabletype=variabletype,axislabel=axislabel,title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f15ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "variabletype = \"mean_delta_for_pairs_with_non-zero_delta\"\n",
    "axislabel = \"Average Change in \"\n",
    "title = 'Average Difference Due to Resilience Investment Among the Smaller Subset of TAZ Pairs Where the \"Resilience\" Case Was Different from the \"No Resilience\" Case)'\n",
    "makebarcharts(variabletype=variabletype,axislabel=axislabel,title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to HTML has moved to TAZ_metrics.py\n",
    "# !jupyter nbconvert MetricsByTAZ.ipynb --to html --no-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_trip_omx_file.close()\n",
    "newdisruptresil_trip_omx_file.close()\n",
    "newdisruptNOresil_trip_omx_file.close()\n",
    "\n",
    "base_skims_omx_file.close()\n",
    "disrupt_noresil_skims_omx_file.close()\n",
    "disrupt_resil_skims_omx_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
