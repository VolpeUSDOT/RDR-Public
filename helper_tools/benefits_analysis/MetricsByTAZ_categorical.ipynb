{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6059d699",
   "metadata": {},
   "source": [
    "# Benefits Analysis at the TAZ Category Level\n",
    "This tool helps the user understand the distribution of benefits of a resilience investment among categories of “transportation analysis zones” (TAZs). This Jupyter notebook takes AequilibraE runs (with and without resilience investment) and outputs an HTML file that reports changes in metrics by category for a user-supplied categorical variable. The user-supplied variable could be any variable of interest that can be associated to TAZs. The tool can be used to explore how impacts and benefits are related to any user-supplied variable, including social equity considerations (e.g., disadvantaged versus baseline community impacts and benefits). Consult the RDR User Guide in C:\\GitHub\\RDR\\documentation for more information on how to use and understand this tool.\n",
    "\n",
    "The default assumption is that the user will run the equity overlay analysis (`run_equity_overlay.bat` file in C:\\GitHub\\RDR\\helper_tools\\benefits_analysis) as a first step, and then use the output from that as an input to this TAZ metrics analysis. However, the user may also directly provide data in a CSV file assigning a variable value to each TAZ from another source, rather than running the equity overlay analysis. If providing other data, the data must be numeric.\n",
    "\n",
    "The purpose is to help the user examine and understand differential impacts of a resilience investment intended to mitigate effects of a disruption, by comparing different TAZ categories of interest. The analysis displays variables to help illuminate the following questions from various angles.\n",
    "\n",
    "**Questions driving this analysis include:**\n",
    "- What is the baseline magnitude of trips, minutes per trip, and miles per trip for each TAZ category?\n",
    "- How relevant is the disruption for each TAZ category?\n",
    "- What is the projected benefit of the resilience investment overall and for each TAZ category, i.e., are the benefits evenly distributed?\n",
    "\n",
    "**TAZ Metrics Configuration**\n",
    "The `TAZ_metrics.config` configuration file allows the user to specify the following:\n",
    "- `path_to_RDR_config_file` – This should identify the location of the configuration file pertinent to the existing RDR Metamodel run and corresponding AequilibraE runs that will be used for this analysis. The analysis will use this configuration file to identify where to access the OMX files from those runs. Additionally, the following AequilibraE scenario dimensions specified in the `TAZ_metrics.config` must correspond to inputs provided for the RDR config in the Model_Parameters.xlsx file and related inputs.\n",
    "- `resil` - Name/label for the resilience project.\n",
    "- `hazard` - Name/label of the hazard event.\n",
    "- `recovery` - Non-negative integer representing level of receding exposure a hazard event may pass through during the hazard exposure period from initial hazard severity to end of the hazard event. \n",
    "- `socio` - Name/label of the socioeconomic future scenario.\n",
    "- `proj_group` - Name/label of the group to which the resilience project belongs.\n",
    "- `elasticity` - Numeric value (less than or equal to zero) quantifying the change in trip demand due to increased travel time.\n",
    "- `run_type` - Defines the type of AequilibraE run used to fit the metamodel. User can select 'SP' for shortest path or 'RT' for routing (default).\n",
    "\n",
    "*Note*: As described above, the default assumption is that the user will use the equity overlay analysis first, and then use the output from that as an input to this TAZ metrics analysis. If the user will instead directly provide the data by TAZ then the user should update the `TAZ_metrics.config` file (or renamed config file referenced in the run_TAZ_metrics.bat file, if applicable) to specify the name of the user-provided file in `TAZ_mapping`. The data must be numeric.\n",
    "\n",
    "## Outputs Review\n",
    "Check output directory for CSV file outputs with underlying data results. This is in the same location as this HTML file, and is the directory specified in the `benefits_analysis_dir` parameter in the TAZ metrics config file.\n",
    "\n",
    "## Charts and Statistical Analysis\n",
    "Scroll down in this HTML file for charts and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openmatrix as omx\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "import equity_config_reader\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../../metamodel_py'))\n",
    "\n",
    "import rdr_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The equity configuration filepath is now passed into this notebook from its parent, TAZ_metrics.py, via the temporary text file,\n",
    "# assuming this notebook and TAZ_metrics.py are both in the same folder.\n",
    "# To run the notebook in isolation, rather than by executing the run_TAZ_metrics.bat file, comment out the below five lines and \n",
    "# uncomment the subsequent two lines.\n",
    "with open('temp.txt', 'r') as f:\n",
    "    config_filepath = f.read()\n",
    "equity_cfg = equity_config_reader.read_equity_config_file(config_filepath)\n",
    "\n",
    "rdr_cfg_path = equity_cfg['path_to_RDR_config_file']\n",
    "\n",
    "# Directory of TAZ metrics helper tool files\n",
    "equity_dir = equity_cfg['benefits_analysis_dir']\n",
    "\n",
    "error_list, cfg = rdr_setup.read_config_file(rdr_cfg_path, 'config')\n",
    "\n",
    "# RDR input parameters\n",
    "RDR_run_id = cfg['run_id']\n",
    "\n",
    "input_dir = cfg['input_dir']\n",
    "\n",
    "# Name of equity variable\n",
    "category_name = equity_cfg['TAZ_feature']\n",
    "\n",
    "# Name of CSV file with equity category value for each TAZ (either output from run_equity_overlay.bat OR user-provided)\n",
    "category_filename = equity_cfg['TAZ_mapping']\n",
    "\n",
    "# Name of the TAZ column\n",
    "TAZ_col_name = equity_cfg['TAZ_col_name']\n",
    "\n",
    "# P-value for use in statistical tests\n",
    "pval = float(equity_cfg['pval'])\n",
    "\n",
    "# Look to see if the equity overlay data exists\n",
    "if not os.path.exists(category_filename):\n",
    "    print('{} not found. Please run the equity_overlay first or generate your own file and specify the filename for it as TAZ_mapping in the TAZ_metrics.config file'.format(category_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c964bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility method for reading OMX files\n",
    "def readOMX(filename, selectedMatrix, debug_mode):\n",
    "    f = omx.open_file(filename)\n",
    "    matrix_size = f.shape()\n",
    "    if debug_mode:\n",
    "        print('Shape: ', f.shape())\n",
    "        print('Number of tables: ', len(f))\n",
    "        print('Table names: ', f.list_matrices())\n",
    "        print('Attributes: ', f.list_all_attributes())\n",
    "    omx_df = f[selectedMatrix]\n",
    "    if debug_mode:\n",
    "        print('Sum of matrix elements: ', '{:.9}'.format(np.sum(omx_df)))\n",
    "        print('Percentiles: ', np.percentile(omx_df, (1, 10, 30, 50, 70, 90, 99)))\n",
    "        print('Maximum: ', np.amax(omx_df))\n",
    "    return omx_df, matrix_size, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for comparison - these come from equity_cfg, the TAZ_metrics.config file\n",
    "resil = equity_cfg['resil']\n",
    "baseline = equity_cfg['baseline']\n",
    "hazard = equity_cfg['hazard']\n",
    "recovery = equity_cfg['recovery']\n",
    "socio = equity_cfg['socio']\n",
    "projgroup = equity_cfg['projgroup']\n",
    "elasticity = equity_cfg['elasticity']\n",
    "elasname = str(int(10 * -elasticity))\n",
    "run_type = equity_cfg['run_type']\n",
    "largeval = float(equity_cfg['largeval'])\n",
    "\n",
    "hours_name = 'free_flow_time'\n",
    "miles_name = 'distance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the \"matrix\" OMX files for \"base\"\n",
    "matrix_omx_folder_path_base = os.path.join(equity_dir, \"aeq_runs\", \"base\", RDR_run_id,\n",
    "                                           socio + projgroup, \"matrix\", \"matrices\")\n",
    "\n",
    "# Location of the \"nocar\" OMX files for \"base\"\n",
    "nocar_omx_folder_path_base = os.path.join(equity_dir, \"aeq_runs\", \"base\", RDR_run_id,\n",
    "                                          socio + projgroup, \"nocar\", \"matrices\")\n",
    "\n",
    "# Location of the \"matrix\" OMX files for \"disruption with resilience investment\"\n",
    "matrix_omx_folder_path = os.path.join(equity_dir, \"aeq_runs\", \"disrupt\", RDR_run_id,\n",
    "                             socio + projgroup + '_' + resil + '_' + elasname + '_' + hazard + '_' + recovery,\n",
    "                             \"matrix\", \"matrices\")\n",
    "\n",
    "# Location of the \"matrix\" OMX files for \"disruption WITHOUT resilience investment\"\n",
    "matrix_omx_folder_path_noresil = os.path.join(equity_dir, \"aeq_runs\", \"disrupt\", RDR_run_id,\n",
    "                                     socio + projgroup + '_' + baseline + '_' + elasname + '_' + hazard + '_' + recovery,\n",
    "                                     \"matrix\", \"matrices\")\n",
    "\n",
    "# Location of the \"nocar\" OMX files for \"base\" and for \"disruption with resilience investment\"\n",
    "nocar_omx_folder_path = os.path.join(equity_dir, \"aeq_runs\", \"disrupt\", RDR_run_id,\n",
    "                             socio + projgroup + '_' + resil + '_' + elasname + '_' + hazard + '_' + recovery,\n",
    "                             \"nocar\", \"matrices\")\n",
    "\n",
    "# Location of the \"nocar\" OMX files for \"disruption WITHOUT resilience investment\"\n",
    "nocar_omx_folder_path_noresil = os.path.join(equity_dir, \"aeq_runs\", \"disrupt\", RDR_run_id,\n",
    "                                     socio + projgroup + '_' + baseline + '_' + elasname + '_' + hazard + '_' + recovery,\n",
    "                                     \"nocar\", \"matrices\")\n",
    "\n",
    "# READING THE TABLES FOR \"MATRIX\"\n",
    "\n",
    "# Read the base OMX trip table\n",
    "matrix_base_matrix_filename = os.path.join(matrix_omx_folder_path_base, 'base_demand_summed.omx')\n",
    "matrix_base_dem, matrix_base_trips_matrix_size, matrix_base_trip_omx_file = readOMX(matrix_base_matrix_filename, 'matrix', 0)\n",
    "df_matrix_base_trips = pd.DataFrame(data=matrix_base_dem)\n",
    "\n",
    "# Read the new OMX trip table in the disruption with resilience case\n",
    "matrix_newdisruptresil_matrix_filename = os.path.join(matrix_omx_folder_path, 'new_demand_summed.omx')\n",
    "matrix_newdisruptresil_dem, matrix_resil_trips_matrix_size, matrix_newdisruptresil_trip_omx_file = readOMX(matrix_newdisruptresil_matrix_filename, 'matrix', 0)\n",
    "df_matrix_resil_trips = pd.DataFrame(data=matrix_newdisruptresil_dem)\n",
    "\n",
    "# Read the new OMX trip table in the disruption WITHOUT resilience case\n",
    "matrix_newdisruptNOresil_matrix_filename = os.path.join(matrix_omx_folder_path_noresil, 'new_demand_summed.omx')\n",
    "matrix_newdisruptNOresil_dem, matrix_noresil_trips_matrix_size, matrix_newdisruptNOresil_trip_omx_file = readOMX(matrix_newdisruptNOresil_matrix_filename, 'matrix', 0)\n",
    "df_matrix_NOresil_trips = pd.DataFrame(data=matrix_newdisruptNOresil_dem)\n",
    "\n",
    "# READING THE TABLES FOR \"NOCAR,\" if applicable\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "\n",
    "    # Read the base OMX trip table\n",
    "    nocar_base_matrix_filename = os.path.join(nocar_omx_folder_path_base, 'base_demand_summed.omx')\n",
    "    nocar_base_dem, nocar_base_trips_matrix_size, nocar_base_trip_omx_file = readOMX(nocar_base_matrix_filename, 'nocar', 0)\n",
    "    df_nocar_base_trips = pd.DataFrame(data=nocar_base_dem)\n",
    "\n",
    "    # Read the new OMX trip table in the disruption with resilience case\n",
    "    nocar_newdisruptresil_matrix_filename = os.path.join(nocar_omx_folder_path, 'new_demand_summed.omx')\n",
    "    nocar_newdisruptresil_dem, nocar_resil_trips_matrix_size, nocar_newdisruptresil_trip_omx_file = readOMX(nocar_newdisruptresil_matrix_filename, 'matrix', 0)\n",
    "    df_nocar_resil_trips = pd.DataFrame(data=nocar_newdisruptresil_dem)\n",
    "\n",
    "    # Read the new OMX trip table in the disruption WITHOUT resilience case\n",
    "    nocar_newdisruptNOresil_matrix_filename = os.path.join(nocar_omx_folder_path_noresil, 'new_demand_summed.omx')\n",
    "    nocar_newdisruptNOresil_dem, nocar_noresil_trips_matrix_size, nocar_newdisruptNOresil_trip_omx_file = readOMX(nocar_newdisruptNOresil_matrix_filename, 'matrix', 0)\n",
    "    df_nocar_NOresil_trips = pd.DataFrame(data=nocar_newdisruptNOresil_dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7665e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of file skims\n",
    "baseskims_filename = run_type + '_' + socio + projgroup\n",
    "disruptskims_noresil_filename = run_type + '_disrupt_' + socio + projgroup + '_' + baseline + '_' + elasname + '_' + hazard + '_' + recovery\n",
    "disruptskims_resil_filename = run_type + '_disrupt_' + socio + projgroup + '_' + resil + '_' + elasname + '_' + hazard + '_' + recovery\n",
    "\n",
    "# Create filepath strings for \"matrix\" tables\n",
    "matrix_baseskims_folder = os.path.join(matrix_omx_folder_path_base, baseskims_filename + '.omx')\n",
    "matrix_disruptskims_noresil_folder = os.path.join(matrix_omx_folder_path_noresil, disruptskims_noresil_filename + '.omx')\n",
    "matrix_disruptskims_resil_folder = os.path.join(matrix_omx_folder_path, disruptskims_resil_filename + '.omx')\n",
    "\n",
    "# Create filepath strings for \"nocar\" tables\n",
    "nocar_baseskims_folder = os.path.join(nocar_omx_folder_path_base, baseskims_filename + '.omx')\n",
    "nocar_disruptskims_noresil_folder = os.path.join(nocar_omx_folder_path_noresil, disruptskims_noresil_filename + '.omx')\n",
    "nocar_disruptskims_resil_folder = os.path.join(nocar_omx_folder_path, disruptskims_resil_filename + '.omx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd566457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING THE BASE SKIMS FOR \"MATRIX\"\n",
    "# Read the base skims OMX for \"matrix\"\n",
    "matrix_base_hours, matrix_base_hours_matrix_size, base_skims_omx_file = readOMX(matrix_baseskims_folder, hours_name, 0)\n",
    "df_matrix_base_hours = pd.DataFrame(data=matrix_base_hours)\n",
    "matrix_base_miles, matrix_base_miles_matrix_size, base_skims_omx_file = readOMX(matrix_baseskims_folder, miles_name, 0)\n",
    "df_matrix_base_miles = pd.DataFrame(data=matrix_base_miles)\n",
    "\n",
    "# READING THE BASE SKIMS FOR \"NOCAR,\" if applicable\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    # Read the base skims OMX for \"nocar\"\n",
    "    nocar_base_hours, nocar_base_hours_matrix_size, base_skims_omx_file = readOMX(nocar_baseskims_folder, hours_name, 0)\n",
    "    df_nocar_base_hours = pd.DataFrame(data=nocar_base_hours)\n",
    "    nocar_base_miles, nocar_base_miles_matrix_size, base_skims_omx_file = readOMX(nocar_baseskims_folder, miles_name, 0)\n",
    "    df_nocar_base_miles = pd.DataFrame(data=nocar_base_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING THE DISRUPTION WITH NO RESILIENCE SKIMS FOR \"MATRIX\"\n",
    "# Read the disrupt skims OMX - no resilience project\n",
    "matrix_disrupt_noresil_hours, matrix_size, disrupt_noresil_skims_omx_file = readOMX(matrix_disruptskims_noresil_folder, hours_name, 0)\n",
    "df_matrix_disrupt_noresil_hours = pd.DataFrame(data=matrix_disrupt_noresil_hours)\n",
    "matrix_disrupt_noresil_miles, matrix_size, disrupt_noresil_skims_omx_file = readOMX(matrix_disruptskims_noresil_folder, miles_name, 0)\n",
    "df_matrix_disrupt_noresil_miles = pd.DataFrame(data=matrix_disrupt_noresil_miles)\n",
    "\n",
    "# READING THE DISRUPTION WITH NO RESILIENCE SKIMS FOR \"NOCAR,\" if applicable\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    # Read the disrupt skims OMX - no resilience project\n",
    "    nocar_disrupt_noresil_hours, matrix_size, disrupt_noresil_skims_omx_file = readOMX(nocar_disruptskims_noresil_folder, hours_name, 0)\n",
    "    df_nocar_disrupt_noresil_hours = pd.DataFrame(data=nocar_disrupt_noresil_hours)\n",
    "    nocar_disrupt_noresil_miles, matrix_size, disrupt_noresil_skims_omx_file = readOMX(nocar_disruptskims_noresil_folder, miles_name, 0)\n",
    "    df_nocar_disrupt_noresil_miles = pd.DataFrame(data=nocar_disrupt_noresil_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING THE DISRUPTION WITH RESILIENCE SKIMS FOR \"MATRIX\"\n",
    "# Read the disrupt skims OMX - with resilience project\n",
    "matrix_disrupt_resil_hours, matrix_size, disrupt_resil_skims_omx_file = readOMX(matrix_disruptskims_resil_folder, hours_name, 0)\n",
    "df_matrix_disrupt_resil_hours = pd.DataFrame(data=matrix_disrupt_resil_hours)\n",
    "matrix_disrupt_resil_miles, matrix_size, disrupt_resil_skims_omx_file = readOMX(matrix_disruptskims_resil_folder, miles_name, 0)\n",
    "df_matrix_disrupt_resil_miles = pd.DataFrame(data=matrix_disrupt_resil_miles)\n",
    "\n",
    "# READING THE DISRUPTION WITH RESILIENCE SKIMS FOR \"NOCAR,\" if applicable\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    # Read the disrupt skims OMX - with resilience project\n",
    "    nocar_disrupt_resil_hours, matrix_size, disrupt_resil_skims_omx_file = readOMX(nocar_disruptskims_resil_folder, hours_name, 0)\n",
    "    df_nocar_disrupt_resil_hours = pd.DataFrame(data=nocar_disrupt_resil_hours)\n",
    "    nocar_disrupt_resil_miles, matrix_size, disrupt_resil_skims_omx_file = readOMX(nocar_disruptskims_resil_folder, miles_name, 0)\n",
    "    df_nocar_disrupt_resil_miles = pd.DataFrame(data=nocar_disrupt_resil_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fa3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create dataframe based on skim results\n",
    "def makeskimresult_df(hours_df,trips_df,miles_df):\n",
    "    # Base times and distances by origin TAZ\n",
    "    # Convert O-D matrix to tall table indexed by origin and destination TAZ\n",
    "    bool_base_hours = hours_df < largeval\n",
    "    a = np.repeat(bool_base_hours.columns, len(bool_base_hours.index))\n",
    "    b = np.tile(bool_base_hours.index, len(bool_base_hours.columns))\n",
    "\n",
    "    # Sums demand where <largeval\n",
    "    base_cumtripcount = (trips_df.where(bool_base_hours, other=0))\n",
    "    base_cumtime = (base_cumtripcount*hours_df)/60\n",
    "    base_cumdist = (base_cumtripcount*miles_df)\n",
    "    c1 = base_cumtripcount.values.ravel()\n",
    "    c2 = base_cumtime.values.ravel()\n",
    "    c3 = base_cumdist.values.ravel()\n",
    "    df = pd.DataFrame({'from':a, 'to':b, 'trips':c1, 'hours':c2, 'miles':c3})\n",
    "    df = df.astype({'from': 'str', 'to': 'str', 'trips': 'int64', 'hours': 'float64', 'miles': 'float64'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframes for \"matrix\"\n",
    "matrix_base_df = makeskimresult_df(df_matrix_base_hours,df_matrix_base_trips,df_matrix_base_miles)\n",
    "matrix_disrupt_noresil_df = makeskimresult_df(df_matrix_disrupt_noresil_hours,df_matrix_NOresil_trips,df_matrix_disrupt_noresil_miles)\n",
    "matrix_disrupt_resil_df = makeskimresult_df(df_matrix_disrupt_resil_hours,df_matrix_resil_trips,df_matrix_disrupt_resil_miles)\n",
    "\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    # Make dataframes for \"nocar,\" if applicable\n",
    "    nocar_base_df = makeskimresult_df(df_nocar_base_hours,df_nocar_base_trips,df_nocar_base_miles)\n",
    "    nocar_disrupt_noresil_df = makeskimresult_df(df_nocar_disrupt_noresil_hours,df_nocar_NOresil_trips,df_nocar_disrupt_noresil_miles)\n",
    "    nocar_disrupt_resil_df = makeskimresult_df(df_nocar_disrupt_resil_hours,df_nocar_resil_trips,df_nocar_disrupt_resil_miles)\n",
    "\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    # If \"nocar\" tables exist, combine \"matrix\" and \"nocar\" dataframes for overall depiction of results\n",
    "    base_df = matrix_base_df.add(nocar_base_df)\n",
    "    disrupt_noresil_df = matrix_disrupt_noresil_df.add(nocar_disrupt_noresil_df)\n",
    "    disrupt_resil_df = matrix_disrupt_resil_df.add(nocar_disrupt_resil_df)\n",
    "else:\n",
    "    # Otherwise if \"nocar\" tables do not exist, the overall results are just those from the \"matrix\" folders\n",
    "    base_df = matrix_base_df\n",
    "    disrupt_noresil_df = matrix_disrupt_noresil_df\n",
    "    disrupt_resil_df = matrix_disrupt_resil_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_base_trip_omx_file.close()\n",
    "matrix_newdisruptresil_trip_omx_file.close()\n",
    "matrix_newdisruptNOresil_trip_omx_file.close()\n",
    "\n",
    "if os.path.exists(nocar_omx_folder_path):\n",
    "    nocar_base_trip_omx_file.close()\n",
    "    nocar_newdisruptresil_trip_omx_file.close()\n",
    "    nocar_newdisruptNOresil_trip_omx_file.close()\n",
    "\n",
    "base_skims_omx_file.close()\n",
    "disrupt_noresil_skims_omx_file.close()\n",
    "disrupt_resil_skims_omx_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d824d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of skim results\n",
    "merged_df = pd.merge(base_df, disrupt_noresil_df, how='inner', on=['from', 'to'], suffixes=(\"_base\", None))\n",
    "taz_pair_skims = pd.merge(merged_df, disrupt_resil_df, how='inner', on=['from', 'to'], suffixes=(\"_disrupt_noresil\", \"_disrupt_resil\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28792866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TAZ ID columns (from and to) to string data type\n",
    "taz_pair_skims['from'] = taz_pair_skims['from'].astype(str)\n",
    "taz_pair_skims['to'] = taz_pair_skims['to'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in equity category label by TAZ\n",
    "taz_equity = pd.read_csv(category_filename,\n",
    "                         usecols=[TAZ_col_name, category_name],\n",
    "                         converters={TAZ_col_name: str, category_name: float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join by from TAZ and to TAZ\n",
    "taz_stats = taz_pair_skims.merge(taz_equity, how='left', left_on='from', right_on=TAZ_col_name).merge(taz_equity, how='left', left_on='to', right_on = TAZ_col_name, suffixes=('_from', '_to'))\n",
    "taz_stats = taz_stats.rename(columns = {TAZ_col_name : 'TAZ'})\n",
    "\n",
    "# Replace NaN values with 'external'. These are for nodes which do not exist in the TAZ file, and therefore do not have any equity attributes. \n",
    "# They are nodes which are outside the MPO boundaries and are needed for travel demand modeling purposes, but do not have shapes associated with them. \n",
    "# They are not omitted because the totals for hours, miles, and trips should be the same at the MPO level as what is reported to users.\n",
    "taz_stats[\n",
    "    [TAZ_col_name + '_from', category_name + '_from', TAZ_col_name + '_to', category_name + '_to']\n",
    "    ] = taz_stats[\n",
    "        [TAZ_col_name + '_from', category_name + '_from', TAZ_col_name + '_to', category_name + '_to']\n",
    "        ].fillna('external')\n",
    "\n",
    "# Calculate relative change in trips/hours/miles for each \n",
    "taz_stats['trips_delta'] = (taz_stats['trips_disrupt_resil'] - taz_stats['trips_disrupt_noresil'])\n",
    "taz_stats['hours_delta'] = (taz_stats['hours_disrupt_resil'] - taz_stats['hours_disrupt_noresil'])\n",
    "taz_stats['miles_delta'] = (taz_stats['miles_disrupt_resil'] - taz_stats['miles_disrupt_noresil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e784805",
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09681775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three variables to flag whether the disruption is relevant for the TAZ pair (for trips/miles/hours)\n",
    "taz_stats['trips_disruption_relevant'] = taz_stats['trips_base'] != taz_stats['trips_disrupt_noresil']\n",
    "taz_stats['hours_disruption_relevant'] = taz_stats['hours_base'] != taz_stats['hours_disrupt_noresil']\n",
    "taz_stats['miles_disruption_relevant'] = taz_stats['miles_base'] != taz_stats['miles_disrupt_noresil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another version of taz_stats that focuses only on the TAZ pairs for which the disruption was relevant\n",
    "filter = taz_stats['trips_disruption_relevant'] == True\n",
    "stats_on_relevant_taz = taz_stats[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c7927",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TAZ_pairs = len(taz_stats)\n",
    "N_TAZ_pairs_impacted = len(stats_on_relevant_taz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0af41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to produce a summary table given an index of interest (i.e., a grouping of TAZ based on equity category of origin/destination).\n",
    "\n",
    "# First create helper function for use in the main function\n",
    "def countnonzeros(x):\n",
    "    return x.astype(bool).sum(axis=0)\n",
    "\n",
    "# Now create main function\n",
    "def createsummary(index, df):\n",
    "    # First address TRIPS\n",
    "    metric = 'trips'\n",
    "    # Aggregate base metric for all three categories (Variables 1T/1H/1M)\n",
    "    category_base = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_base':\"sum\"}, \n",
    "                                    fill_value=0)\n",
    "    category_base = pd.DataFrame(category_base.to_records())\n",
    "    # Create variables 2aT/2aH/2aM: Percent change from baseline due to disruption (without resilience investment) and merge with the prior into one dataframe\n",
    "    Q_TwoA = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_disrupt_noresil':\"sum\"}, \n",
    "                                    fill_value=0)\n",
    "    category_stats = pd.merge(category_base,Q_TwoA,on=None,left_index=True, right_index=True)\n",
    "    category_stats[metric+'_absolute_change_noresil'] = category_stats[metric+'_disrupt_noresil'] - category_stats[metric+'_base']\n",
    "    category_stats[metric+'_percent_change_noresil'] = (category_stats[metric+'_absolute_change_noresil']*100)/category_stats[metric+'_base']\n",
    "\n",
    "    # Create variables 2bT/2bH/2bM: Percent of TAZ with a change in metric due to disruption (without resilience investment)\n",
    "    Q_TwoB = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_disruption_relevant':[countnonzeros,len]}, \n",
    "                                    fill_value=0)\n",
    "    # Flatten multi index hierarchy in column headers\n",
    "    Q_TwoB = pd.DataFrame(Q_TwoB.to_records())\n",
    "    Q_TwoB[metric+'_percent_TAZ_relevant'] = (Q_TwoB[\"('\"+metric+\"_disruption_relevant', 'countnonzeros')\"]*100)/Q_TwoB[\"('\"+metric+\"_disruption_relevant', 'len')\"]\n",
    "    # Merge into category_stats dataframe\n",
    "    category_stats = pd.merge(category_stats,Q_TwoB,on=index)\n",
    "\n",
    "    # Create variables 3aT/3aH/3aM: Overall impact of resilience investment (i.e., metric in the \"resilience\" case minus metric in the \"no resilience\" case)\n",
    "    Q_ThreeA = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_disrupt_resil':\"sum\"}, \n",
    "                                    fill_value=0)\n",
    "    category_stats = pd.merge(category_stats,Q_ThreeA,on=index)\n",
    "    category_stats[metric+\"_delta_absolute\"] = category_stats[metric+'_disrupt_resil'] - category_stats[metric+'_disrupt_noresil']\n",
    "    # Create variables 3bT/3bH/3bM: Same as the above set, except divided by the metric in the \"no resilience\" case and multiplied by 100 to show percent change relative to \"no resilience\" case\n",
    "    category_stats[metric+'_delta_relative'] = (category_stats[metric+'_delta_absolute']*100)/category_stats[metric+'_disrupt_noresil']\n",
    "\n",
    "    # Average difference in metrics due to resilience investment for ALL TAZ \n",
    "    AvgDeltaforAll = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_delta':\"mean\"}, \n",
    "                                    fill_value=0)\n",
    "    AvgDeltaforAll = AvgDeltaforAll.rename(columns = {metric+'_delta':metric+'_mean_delta_for_ALL'})\n",
    "    if metric+'_mean_delta_for_ALL' in AvgDeltaforAll.columns.to_list():\n",
    "        category_stats = pd.merge(category_stats,AvgDeltaforAll,on=index)\n",
    "\n",
    "    # Create variables 3cT/3cH/3cM: Average difference in metric due to resilience investment for all relevant TAZ pairs (i.e., among the subset of TAZ pairs where there was a disruption impact in the \"no resilience\" case)\n",
    "    # First filter for relevant TAZ pairs\n",
    "    relevant_filter = df[metric+'_disrupt_noresil'] != df[metric+'_base'] \n",
    "    relevant_set = df[relevant_filter]\n",
    "    Q_ThreeC = pd.pivot_table(relevant_set, index=index, values=relevant_set.columns.to_list(),\n",
    "                                    aggfunc={metric+'_delta':\"mean\"}, \n",
    "                                    fill_value=0)\n",
    "    Q_ThreeC = Q_ThreeC.rename(columns = {metric+'_delta':metric+'_mean_delta_for_relevant_pairs'})\n",
    "    if metric+'_mean_delta_for_relevant_pairs' in Q_ThreeC.columns.to_list():\n",
    "        category_stats = pd.merge(category_stats,Q_ThreeC,on=index)\n",
    "\n",
    "    # Create variables 3dT/3dH/3dM: Average difference in metric due to resilience investment for all TAZ pairs with non-zero delta due to resilience (i.e., among the even smaller subset of TAZ pairs where the \"resilience\" case was different from the \"no resilience\" case)\n",
    "    nonzerodelta_filter = df[metric+'_disrupt_noresil'] != df[metric+'_disrupt_resil']\n",
    "    nonzerodelta_set = df[nonzerodelta_filter]\n",
    "    Q_ThreeD = pd.pivot_table(nonzerodelta_set, index=index, values=nonzerodelta_set.columns.to_list(),\n",
    "                                aggfunc={metric+'_delta':\"mean\"}, \n",
    "                                fill_value=0)\n",
    "    Q_ThreeD = Q_ThreeD.rename(columns = {metric+'_delta':metric+'_mean_delta_for_pairs_with_non-zero_delta'})\n",
    "    if metric+'_mean_delta_for_pairs_with_non-zero_delta' in Q_ThreeD.columns.to_list():\n",
    "        category_stats = pd.merge(category_stats,Q_ThreeD,on=index)\n",
    "\n",
    "\n",
    "    # HOURS PER TRIP\n",
    "    metric = 'hours'\n",
    "    hours_stats = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_base':\"sum\"}, \n",
    "                                    fill_value=0)    \n",
    "    hours_stats = pd.DataFrame(hours_stats.to_records())\n",
    "    # Pull in necessary prerequisite values for hours\n",
    "    Q_TwoA = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_disrupt_noresil':\"sum\"}, \n",
    "                                    fill_value=0)\n",
    "    hours_stats = pd.merge(hours_stats,Q_TwoA,on=index)\n",
    "\n",
    "    Q_ThreeA = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_disrupt_resil':\"sum\"}, \n",
    "                                    fill_value=0)\n",
    "    hours_stats = pd.merge(hours_stats,Q_ThreeA,on=index)\n",
    "\n",
    "    # Merge into category_stats\n",
    "    category_stats = pd.merge(category_stats,hours_stats,on=None,left_index=True, right_index=True)\n",
    "    \n",
    "    # Create variables 2aT/2aH/2aM: Percent change from baseline \"minutes per trip\" due to disruption (without resilience investment)\n",
    "    category_stats['minutespertrip_base'] = (category_stats[metric+'_base']*60)/category_stats['trips_base']\n",
    "    category_stats['minutespertrip_disrupt_noresil'] = (category_stats[metric+'_disrupt_noresil']*60)/category_stats['trips_disrupt_noresil']\n",
    "    \n",
    "    category_stats['minutespertrip_absolute_change_noresil'] = category_stats['minutespertrip_disrupt_noresil'] - category_stats['minutespertrip_base']\n",
    "    category_stats['minutespertrip_percent_change_noresil'] = (category_stats['minutespertrip_absolute_change_noresil']*100)/category_stats['minutespertrip_base']    \n",
    "    \n",
    "    category_stats['minutespertrip_disrupt_resil'] = (category_stats[metric+'_disrupt_resil']*60)/category_stats['trips_disrupt_resil']\n",
    "    \n",
    "    # No need to do question 2B (percent TAZ relevant) because we would have already captured this with trips\n",
    "\n",
    "    # Create variables 3aT/3aH/3aM: Overall impact of resilience investment in minutes per trip (i.e., minutes per trip in the \"resilience\" case minus minutes per trip in the \"no resilience\" case)\n",
    "    category_stats[\"minutespertrip_delta_absolute\"] = category_stats['minutespertrip_disrupt_resil'] - category_stats['minutespertrip_disrupt_noresil']\n",
    "    \n",
    "    # Create variables 3bT/3bH/3bM: Same as the above set, except divided by the minutes per trip in the \"no resilience\" case and multiplied by 100 to show percent change relative to \"no resilience\" case\n",
    "    category_stats['minutespertrip_delta_relative'] = (category_stats['minutespertrip_delta_absolute']*100)/category_stats['minutespertrip_disrupt_noresil']\n",
    "\n",
    "    # MILES PER TRIP\n",
    "    metric = 'miles'\n",
    "    miles_stats = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_base':\"sum\"}, \n",
    "                                    fill_value=0)    \n",
    "    miles_stats = pd.DataFrame(miles_stats.to_records())\n",
    "    # Pull in necessary prerequisite values for miles\n",
    "    Q_TwoA = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_disrupt_noresil':\"sum\"}, \n",
    "                                    fill_value=0)\n",
    "    miles_stats = pd.merge(miles_stats,Q_TwoA,on=index)\n",
    "\n",
    "    Q_ThreeA = pd.pivot_table(df, index=index, values=df.columns.to_list(),\n",
    "                                    aggfunc={metric+'_disrupt_resil':\"sum\"}, \n",
    "                                    fill_value=0)\n",
    "    miles_stats = pd.merge(miles_stats,Q_ThreeA,on=index)\n",
    "\n",
    "    # Merge into category_stats\n",
    "    category_stats = pd.merge(category_stats,miles_stats,on=None,left_index=True, right_index=True)\n",
    "    \n",
    "    # Create variables 2aT/2aH/2aM: Percent change from baseline \"miles per trip\" due to disruption (without resilience investment)\n",
    "    category_stats['milespertrip_base'] = (category_stats[metric+'_base'])/category_stats['trips_base']\n",
    "    category_stats['milespertrip_disrupt_noresil'] = (category_stats[metric+'_disrupt_noresil'])/category_stats['trips_disrupt_noresil']\n",
    "\n",
    "    category_stats['milespertrip_absolute_change_noresil'] = category_stats['milespertrip_disrupt_noresil'] - category_stats['milespertrip_base']\n",
    "    category_stats['milespertrip_percent_change_noresil'] = (category_stats['milespertrip_absolute_change_noresil']*100)/category_stats['milespertrip_base']            \n",
    "\n",
    "    category_stats['milespertrip_disrupt_resil'] = (category_stats[metric+'_disrupt_resil'])/category_stats['trips_disrupt_resil']\n",
    "\n",
    "    # No need to do question 2B (percent TAZ relevant) because we would have already captured this with trips\n",
    "\n",
    "    # Create variables 3aT/3aH/3aM: Overall impact of resilience investment in miles per trip (i.e., miles per trip in the \"resilience\" case minus miles minutes per trip in the \"no resilience\" case)\n",
    "    category_stats[\"milespertrip_delta_absolute\"] = category_stats['milespertrip_disrupt_resil'] - category_stats['milespertrip_disrupt_noresil']\n",
    "    \n",
    "    # Create variables 3bT/3bH/3bM: Same as the above set, except divided by the miles per trip in the \"no resilience\" case and multiplied by 100 to show percent change relative to \"no resilience\" case\n",
    "    category_stats['milespertrip_delta_relative'] = (category_stats['milespertrip_delta_absolute']*100)/category_stats['milespertrip_disrupt_noresil']\n",
    "    \n",
    "    # Subset the columns of interest\n",
    "    category_stats = category_stats.filter([index,\n",
    "                                            'trips_base', \n",
    "\n",
    "                                            'trips_disrupt_noresil',\n",
    "                                            'trips_disrupt_resil',\n",
    "                                            #\"('trips_disruption_relevant', 'countnonzeros')\"\n",
    "                                            #\"('trips_disruption_relevant', 'len')\"\n",
    "                                            \n",
    "                                            'trips_absolute_change_noresil',\n",
    "                                            'trips_percent_change_noresil',\n",
    "                                            'trips_percent_TAZ_relevant',\n",
    "                                            'trips_delta_absolute', \n",
    "                                            'trips_delta_relative',\n",
    "\n",
    "                                            'trips_mean_delta_for_ALL',\n",
    "\n",
    "                                            'trips_mean_delta_for_relevant_pairs',\n",
    "\n",
    "                                            'trips_mean_delta_for_pairs_with_non-zero_delta',\n",
    "                                            \n",
    "                                            'hours_base', \n",
    "                                            'hours_disrupt_noresil',\n",
    "                                            'hours_disrupt_resil',\n",
    "                                            \n",
    "                                            'minutespertrip_base', \n",
    "                                            'minutespertrip_absolute_change_noresil',\n",
    "                                            'minutespertrip_percent_change_noresil', \n",
    "                                            'minutespertrip_delta_absolute', \n",
    "                                            'minutespertrip_delta_relative',\n",
    "                                            \n",
    "                                            'miles_base', \n",
    "                                            'miles_disrupt_noresil',\n",
    "                                            'miles_disrupt_resil',\n",
    "                                                                                        \n",
    "                                            'milespertrip_base', \n",
    "                                            'milespertrip_absolute_change_noresil',\n",
    "                                            'milespertrip_percent_change_noresil', \n",
    "                                            'milespertrip_delta_absolute', \n",
    "                                            'milespertrip_delta_relative'],\n",
    "                                       axis=1)\n",
    "\n",
    "    # Convert the equity category to string for better rendering in the charts that follow.\n",
    "    category_stats[index] = category_stats[index].astype(str)\n",
    "    return category_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper function that does createsummary twice -\n",
    "# First with the full data set -\n",
    "# Then a second time with the subset of TAZ for which the disruption is relevant.\n",
    "# Then it joins the columns for the second version with those of the first so they will be included in the export as well as available for \n",
    "# The charts below.\n",
    "def combine_summary(index):\n",
    "    full_summary = createsummary(index = index, df = taz_stats)\n",
    "    relevant_taz_summary = createsummary(index = index, df = stats_on_relevant_taz)\n",
    "    combined_summary = full_summary.merge(relevant_taz_summary, how='inner', on=index, suffixes=(None, \"_rel_taz\"))\n",
    "    return combined_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce summary table for trips, minutes per trip, and miles per trip\n",
    "summary = combine_summary(category_name + '_from')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another function to aggregate and calculate metrics by TAZ of origin or destination - \n",
    "# This one is less aggregated than the \"create summary\" function above. Instead of each row\n",
    "# being a equity category, each row is a TAZ and trips are assigned either by TAZ of origin or \n",
    "# TAZ of destination. The sole purpose of this is to produce .csv outputs for the user that \n",
    "# parallel the .csv outputs that the user would get from the continuous version of the notebook.\n",
    "def aggregate(from_or_to):\n",
    "    summary = pd.pivot_table(taz_pair_skims, index=from_or_to, values=taz_pair_skims.columns.to_list(),\n",
    "                                      aggfunc={'trips_base':\"sum\",\n",
    "                                               'trips_disrupt_noresil':\"sum\",\n",
    "                                               'trips_disrupt_resil': \"sum\",\n",
    "                                               'hours_base':\"sum\",\n",
    "                                               'hours_disrupt_noresil':\"sum\",\n",
    "                                               'hours_disrupt_resil': \"sum\",\n",
    "                                               'miles_base':\"sum\",\n",
    "                                               'miles_disrupt_noresil':\"sum\",\n",
    "                                               'miles_disrupt_resil': \"sum\",\n",
    "                                               }, \n",
    "                                               fill_value=0)\n",
    "    summary = pd.DataFrame(summary.to_records())\n",
    "\n",
    "    # MINUTES PER TRIP calculations\n",
    "    summary['minutespertrip_base'] = (summary['hours_base']*60)/summary['trips_base']\n",
    "    summary['minutespertrip_disrupt_noresil'] = (summary['hours_disrupt_noresil']*60)/summary['trips_disrupt_noresil']\n",
    "    summary['minutespertrip_disrupt_resil'] = (summary['hours_disrupt_resil']*60)/summary['trips_disrupt_resil']\n",
    "    # MILES PER TRIP calculations\n",
    "    summary['milespertrip_base'] = (summary['miles_base'])/summary['trips_base']\n",
    "    summary['milespertrip_disrupt_noresil'] = (summary['miles_disrupt_noresil'])/summary['trips_disrupt_noresil']\n",
    "    summary['milespertrip_disrupt_resil'] = (summary['miles_disrupt_resil'])/summary['trips_disrupt_resil']    \n",
    "    # Additional trip calculations\n",
    "    metric = \"trips\"\n",
    "    summary[metric+'_percent_change_noresil'] = ((summary[metric+'_disrupt_noresil'] - summary[metric+'_base'])*100)/summary[metric+'_base']\n",
    "    summary[metric+\"_delta_absolute\"] = summary[metric+'_disrupt_resil'] - summary[metric+'_disrupt_noresil']\n",
    "    summary[metric+'_delta_relative'] = (summary[metric+'_delta_absolute']*100)/summary[metric+'_disrupt_noresil']\n",
    "    # Additional minutes per trip calculations\n",
    "    metric = \"minutespertrip\"\n",
    "    summary[metric+'_percent_change_noresil'] = ((summary[metric+'_disrupt_noresil'] - summary[metric+'_base'])*100)/summary[metric+'_base']\n",
    "    summary[metric+\"_delta_absolute\"] = summary[metric+'_disrupt_resil'] - summary[metric+'_disrupt_noresil']\n",
    "    summary[metric+'_delta_relative'] = (summary[metric+'_delta_absolute']*100)/summary[metric+'_disrupt_noresil']\n",
    "    # Additional miles per trip calculations\n",
    "    metric = \"milespertrip\"\n",
    "    summary[metric+'_percent_change_noresil'] = ((summary[metric+'_disrupt_noresil'] - summary[metric+'_base'])*100)/summary[metric+'_base']\n",
    "    summary[metric+\"_delta_absolute\"] = summary[metric+'_disrupt_resil'] - summary[metric+'_disrupt_noresil']\n",
    "    summary[metric+'_delta_relative'] = (summary[metric+'_delta_absolute']*100)/summary[metric+'_disrupt_noresil']    \n",
    "\n",
    "    # Join by 'from' TAZ or 'to' TAZ (as the case may be)\n",
    "    summary = summary.merge(taz_equity, how='left', left_on=from_or_to, right_on=TAZ_col_name)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First produce a CSV output at the most aggregated level (rows are equity category)\n",
    "csv_summary_filepath = os.path.join(equity_dir,\"MetricsByTAZ_summary_{}_byTAZCategory.csv\".format(equity_cfg['run_id']))\n",
    "# Produce a summary CSV file\n",
    "summary.to_csv(csv_summary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c796ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next produce two CSV outputs that are a bit less aggregated (rows are TAZ and trips grouped by either origin or destination TAZ)\n",
    "\n",
    "# By origin TAZ\n",
    "TAZ_origin_stats = aggregate('from')\n",
    "origin_csv_summary_filepath = os.path.join(equity_dir,\"MetricsByTAZ_summary_{}_byTAZofOrigin.csv\".format(equity_cfg['run_id']))\n",
    "# Produce a summary CSV file\n",
    "TAZ_origin_stats.to_csv(origin_csv_summary_filepath)\n",
    "\n",
    "# By destination TAZ\n",
    "TAZ_destination_stats = aggregate('to')\n",
    "destination_csv_summary_filepath = os.path.join(equity_dir,\"MetricsByTAZ_summary_{}_byTAZofDestination.csv\".format(equity_cfg['run_id']))\n",
    "# Produce a summary CSV file\n",
    "TAZ_destination_stats.to_csv(destination_csv_summary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdabeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prerequisite dictionaries and list to use the functions that create charts\n",
    "ylabel_dict = {\"percent_change_noresil\":\"Percent Change in \",\n",
    "\"delta_absolute\":\"Change in \",\n",
    "\"delta_relative\":\"Percent Change in \",\n",
    "\"base\":\"\",\n",
    "\"percent_TAZ_relevant\":\"Percent of TAZ\"}\n",
    "\n",
    "title_dict = {\"percent_change_noresil\":\"Percent Change from Baseline Due to Disruption (without Resilience) (i.e., Computing Difference in Metric Compared to Base Value, Then Dividing by Base Value)\",\n",
    "\"delta_absolute\":\"Overall Impact of Resilience Investment as Compared to 'No Resilience' Case, for All TAZ (i.e., Metric in 'Resilience' Case Minus Metric in 'No Resilience' Case)\",\n",
    "\"delta_relative\":\"Relative Impact of Resilience Investment as Compared to 'No Resilience' Case, for All TAZ (i.e., Overall Impact Divided by Value of Metric in 'No Resilience' Case)\",\n",
    "\"base\":\"Baseline Magnitude of Metrics for Each Category, Absent Disruption\",\n",
    "\"percent_TAZ_relevant\":\"Percent of TAZ with Potential Impacts from Disruption\"}\n",
    "\n",
    "# Refer to these URLs to an interactive color-picker tool for a view on the color swaths selected for each variable type (and in case updates are needed in the future):\n",
    "# \"base\": https://tristen.ca/hcl-picker/#/hlc/19/1/15534C/E2E062\n",
    "# \"percent_change_noresil\": https://tristen.ca/hcl-picker/#/hlc/19/1/2A3949/50E7ED\n",
    "# \"percent_TAZ_relevant\": https://tristen.ca/hcl-picker/#/hlc/19/1/372230/C7B1EA\n",
    "# \"delta_absolute\": https://tristen.ca/hcl-picker/#/hlc/19/1.65/521916/000000\n",
    "# \"delta_relative\": https://tristen.ca/hcl-picker/#/hlc/19/1.27/43210E/D58437\n",
    "# Each list of colors includes 19, because that is the maximum number of unique values in the user-supplied variable for the tool to run this categorical version of the notebook.\n",
    "\n",
    "color_dict = {\"percent_change_noresil\":[\"#2A3949\",\"#2E4153\",\"#324A5C\",\"#365266\",\"#395B6F\",\"#3C6479\",\"#3F6D82\",\"#41778C\",\"#438096\",\"#458A9F\",\"#4794A8\",\"#489EB1\",\"#4AA8BA\",\"#4BB2C3\",\"#4CBDCC\",\"#4DC7D4\",\"#4DD2DD\",\"#4EDDE5\",\"#50E7ED\"],\n",
    "\"delta_absolute\":[\"#521916\",\"#5B1C1A\",\"#65201F\",\"#6E2323\",\"#782628\",\"#822A2D\",\"#8C2D33\",\"#963138\",\"#A0343E\",\"#AA3844\",\"#B43B4A\",\"#BF3F51\",\"#C94357\",\"#D4465E\",\"#DE4A66\",\"#E94E6D\",\"#F45275\",\"#FE567C\",\"#FC7387\"],\n",
    "\"delta_relative\":[\"#43210E\",\"#4B2610\",\"#522B13\",\"#5A3015\",\"#623417\",\"#6A3A1A\",\"#723F1C\",\"#7A441E\",\"#824921\",\"#8A4F23\",\"#925425\",\"#9B5A28\",\"#A3602A\",\"#AB652C\",\"#B36B2E\",\"#BC7130\",\"#C47732\",\"#CC7D35\",\"#D58437\"],\n",
    "\"base\":[\"#15534C\",\"#195B50\",\"#1F6354\",\"#256C57\",\"#2C745A\",\"#347C5C\",\"#3D855E\",\"#478D5F\",\"#529560\",\"#5D9D61\",\"#69A561\",\"#76AD61\",\"#83B561\",\"#91BD61\",\"#A0C460\",\"#B0CC60\",\"#C0D360\",\"#D1D961\",\"#E2E062\"],\n",
    "\"percent_TAZ_relevant\":[\"#372230\",\"#402838\",\"#482F41\",\"#50364A\",\"#593D53\",\"#61445D\",\"#6A4B66\",\"#725370\",\"#7A5B7A\",\"#826285\",\"#8B6B8F\",\"#93739A\",\"#9A7BA5\",\"#A284B0\",\"#AA8CBB\",\"#B195C7\",\"#B99ED2\",\"#C0A7DE\",\"#C7B1EA\"]}\n",
    "\n",
    "metrics_list = [\"base\",\n",
    "\"percent_change_noresil\",\n",
    "\"delta_absolute\",\n",
    "\"delta_relative\",\n",
    "\"percent_TAZ_relevant\"]\n",
    "\n",
    "y_hoverformat_dict = {\"percent_change_noresil\":\"%{y:.3}%\",\n",
    "\"delta_absolute\":\"%{y:,.5}\",\n",
    "\"delta_relative\":\"%{y:.3}%\",\n",
    "\"base\":\"%{y:,.7}\",\n",
    "\"percent_TAZ_relevant\":\"%{y:.3}%\"}\n",
    "\n",
    "label_dict = {\"percent_change_noresil\":'{:.2f}%',\n",
    "\"delta_absolute\":'{:,.2f}',\n",
    "\"delta_relative\":'{:.2f}%',\n",
    "\"base\":'{:,.2f}',\n",
    "\"percent_TAZ_relevant\":'{:.2f}%'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6280eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to generate bar charts for each question of interest\n",
    "def makebarcharts(variabletype):\n",
    "    if \"trips_\"+variabletype in summary.columns.to_list():\n",
    "        if (summary['trips_'+variabletype] == 0).all() and (summary['minutespertrip_'+variabletype] == 0).all() and (summary['milespertrip_'+variabletype] == 0).all():\n",
    "            print(\"The '___{}' variable was zero for all categories for trips, minutes per trip, and miles per trip, so no chart was produced.\".format(variabletype))\n",
    "        else:\n",
    "            # Establish color list for the plots based on number of categories in user-supplied variable\n",
    "            if taz_equity[category_name].nunique() == 1:\n",
    "                color_list = color_dict[variabletype][0]\n",
    "            elif taz_equity[category_name].nunique() == 2:\n",
    "                color_list = color_dict[variabletype][0::18]\n",
    "            else:\n",
    "                # Get the estimated interval for systematic sampling of the colors list by dividing 19 (length of list) by the number of unique values (i.e., categories) in the user-supplied variable minus 1.\n",
    "                interval = 19//(taz_equity[category_name].nunique() - 1)\n",
    "                # Take every nth item based on the interval\n",
    "                color_list = color_dict[variabletype][0::interval]\n",
    "            # Now make plots\n",
    "            fig = make_subplots(rows=1, cols=3)\n",
    "            y_for_all_TAZ = [summary[\"trips_\"+variabletype], summary[\"minutespertrip_\"+variabletype], summary[\"milespertrip_\"+variabletype]]\n",
    "            texts_all = [summary[\"trips_\"+variabletype].map(label_dict[variabletype].format),\n",
    "                         summary[\"minutespertrip_\"+variabletype].map(label_dict[variabletype].format),\n",
    "                         summary[\"milespertrip_\"+variabletype].map(label_dict[variabletype].format)]\n",
    "            fig.add_trace(go.Bar(\n",
    "                y= summary[\"trips_\"+variabletype],\n",
    "                x= summary[category_name + '_from'],\n",
    "                marker=dict(color=color_list),\n",
    "                hovertemplate=\n",
    "                \"Category of Origin TAZ (\"+category_name+\")\"+\": %{x:.1f}<br>\" +\n",
    "                ylabel_dict[variabletype]+\"Trips: \"+y_hoverformat_dict[variabletype]+\"<br>\" +\n",
    "                \"<extra></extra>\",\n",
    "                #text= summary[\"trips_\"+variabletype].map(label_dict[variabletype].format)\n",
    "                text= summary[\"trips_\"+variabletype].map(label_dict[variabletype].format)\n",
    "                ),\n",
    "                row=1, col=1)\n",
    "            fig.add_trace(go.Bar(\n",
    "                y= summary[\"minutespertrip_\"+variabletype],\n",
    "                x= summary[category_name + '_from'],\n",
    "                marker=dict(color=color_list),\n",
    "                hovertemplate=\n",
    "                \"Category of Origin TAZ (\"+category_name+\")\"+\": %{x:.1f}<br>\" +\n",
    "                ylabel_dict[variabletype]+\"Minutes per Trip: \"+y_hoverformat_dict[variabletype]+\"<br>\" +\n",
    "                \"<extra></extra>\",\n",
    "                text= summary[\"minutespertrip_\"+variabletype].map(label_dict[variabletype].format)),\n",
    "                row=1, col=2)\n",
    "            fig.add_trace(go.Bar(\n",
    "                y= summary[\"milespertrip_\"+variabletype],\n",
    "                x= summary[category_name + '_from'],\n",
    "                marker=dict(color=color_list),\n",
    "                hovertemplate=\n",
    "                \"Category of Origin TAZ (\"+category_name+\")\"+\": %{x:.1f}<br>\" +\n",
    "                ylabel_dict[variabletype]+\"Miles per Trip: \"+y_hoverformat_dict[variabletype]+\"<br>\" +\n",
    "                \"<extra></extra>\",\n",
    "                text= summary[\"milespertrip_\"+variabletype].map(label_dict[variabletype].format)),\n",
    "                row=1, col=3)\n",
    "            ############## Below are the versions of the traces that focus only on the TAZ where the disruption was relevant #######################\n",
    "            y_for_rel_TAZ = [summary[\"trips_\"+variabletype+\"_rel_taz\"], summary[\"minutespertrip_\"+variabletype+\"_rel_taz\"], summary[\"milespertrip_\"+variabletype+\"_rel_taz\"]]\n",
    "            texts_rel = [summary[\"trips_\"+variabletype+\"_rel_taz\"].map(label_dict[variabletype].format),\n",
    "                         summary[\"minutespertrip_\"+variabletype+\"_rel_taz\"].map(label_dict[variabletype].format),\n",
    "                         summary[\"milespertrip_\"+variabletype+\"_rel_taz\"].map(label_dict[variabletype].format)]\n",
    "            fig.add_trace(go.Bar(\n",
    "                y= summary[\"trips_\"+variabletype+\"_rel_taz\"],\n",
    "                x= summary[category_name + '_from'],\n",
    "                marker=dict(color=color_list),\n",
    "                hovertemplate=\n",
    "                \"Category of Origin TAZ (\"+category_name+\")\"+\": %{x:.1f}<br>\" +\n",
    "                ylabel_dict[variabletype]+\"Trips (for relevant TAZ): \"+y_hoverformat_dict[variabletype]+\"<br>\" +\n",
    "                \"<extra></extra>\",\n",
    "                #text= summary[\"trips_\"+variabletype+\"_rel_taz\"].map(label_dict[variabletype].format)\n",
    "                text= summary[\"trips_\"+variabletype+\"_rel_taz\"].map(label_dict[variabletype].format),\n",
    "                visible=False),\n",
    "                row=1, col=1)\n",
    "            fig.add_trace(go.Bar(\n",
    "                y= summary[\"minutespertrip_\"+variabletype+\"_rel_taz\"],\n",
    "                x= summary[category_name + '_from'],\n",
    "                marker=dict(color=color_list),\n",
    "                hovertemplate=\n",
    "                \"Category of Origin TAZ (\"+category_name+\")\"+\": %{x:.1f}<br>\" +\n",
    "                ylabel_dict[variabletype]+\"Minutes per Trip (for relevant TAZ): \"+y_hoverformat_dict[variabletype]+\"<br>\" +\n",
    "                \"<extra></extra>\",\n",
    "                text= summary[\"minutespertrip_\"+variabletype+\"_rel_taz\"].map(label_dict[variabletype].format),\n",
    "                visible=False),\n",
    "                row=1, col=2)\n",
    "            fig.add_trace(go.Bar(\n",
    "                y= summary[\"milespertrip_\"+variabletype+\"_rel_taz\"],\n",
    "                x= summary[category_name + '_from'],\n",
    "                marker=dict(color=color_list),\n",
    "                hovertemplate=\n",
    "                \"Category of Origin TAZ (\"+category_name+\")\"+\": %{x:.1f}<br>\" +\n",
    "                ylabel_dict[variabletype]+\"Miles per Trip (for relevant TAZ): \"+y_hoverformat_dict[variabletype]+\"<br>\" +\n",
    "                \"<extra></extra>\",\n",
    "                text= summary[\"milespertrip_\"+variabletype+\"_rel_taz\"].map(label_dict[variabletype].format),\n",
    "                visible=False),\n",
    "                row=1, col=3)\n",
    "            #############################################################################################\n",
    "            # edit axis labels\n",
    "            fig['layout']['xaxis']['title']=\"Category of Origin TAZ (\"+category_name+\")\"\n",
    "            fig['layout']['xaxis2']['title']=\"Category of Origin TAZ (\"+category_name+\")\"\n",
    "            fig['layout']['xaxis3']['title']=\"Category of Origin TAZ (\"+category_name+\")\"  \n",
    "            fig['layout']['yaxis']['title']=ylabel_dict[variabletype]+\"Trips\"\n",
    "            fig['layout']['yaxis2']['title']=ylabel_dict[variabletype]+\"Minutes per Trip\"\n",
    "            fig['layout']['yaxis3']['title']=ylabel_dict[variabletype]+\"Miles per Trip\"\n",
    "\n",
    "            # Update title and height\n",
    "            fig.update_layout(title_text=title_dict[variabletype], height=700, showlegend=False)\n",
    "\n",
    "            # Add dropdown menu to toggle back and forth between showing all TAZ and just the subset of TAZ for which the disruption is relevant.\n",
    "            fig.update_layout(\n",
    "                updatemenus=[\n",
    "                    dict(\n",
    "                        buttons=[\n",
    "                            dict(\n",
    "                                args=[{'y': y_for_all_TAZ,\n",
    "                                      'text' : texts_all}],\n",
    "                                #args=[{'visible':True},[0,1,2]], \n",
    "                                label=\"All TAZ pairs (n={:,})\".format(N_TAZ_pairs),\n",
    "                                method=\"update\"\n",
    "                            ),\n",
    "                            dict(\n",
    "                                args=[{'y': y_for_rel_TAZ,\n",
    "                                      'text' : texts_rel}],\n",
    "                                      #{\"labels\": texts_rel}],\n",
    "                                #args=[{'visible':True},[3,4,5]],  \n",
    "                                label=\"Subset of TAZ Pairs Impacted by Disruption (n={:,})\".format(N_TAZ_pairs_impacted),\n",
    "                                method=\"update\"\n",
    "                            )],\n",
    "                        showactive=True,\n",
    "                        x=0,\n",
    "                        xanchor=\"left\",\n",
    "                        y=1.08,\n",
    "                        yanchor=\"top\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb82ba",
   "metadata": {},
   "source": [
    "## Questions and corresponding variables\n",
    "### Question 1: What is the baseline magnitude of the metric for each TAZ category?\n",
    "- Variables: Overall metric for each TAZ category absent disruption\n",
    "    - `trips_base`\n",
    "    - `minutespertrip_base`\n",
    "    - `milespertrip_base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "makebarcharts(\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a582fd",
   "metadata": {},
   "source": [
    "    \n",
    "### Question 2: How relevant is the disruption for each TAZ category?\n",
    "##### Question 2A: What was the relative change in the travel metrics due to disruption without resilience?\n",
    "- Variables: Percent change from baseline metric (number of trips, minutes per trip, miles per trip) due to disruption (without resilience investment) by TAZ category\n",
    "    - `trips_percent_change_noresil`\n",
    "    - `minutespertrip_percent_change_noresil`\n",
    "    - `milespertrip_percent_change_noresil` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67936f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "makebarcharts(\"percent_change_noresil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be22f34",
   "metadata": {},
   "source": [
    "##### Question 2B: What percent of TAZs were impacted by the disruption without resilience?\n",
    "- Variable 2bT: Percent of TAZs with a change in number of trips due to disruption (without resilience investment) by TAZ category\n",
    "    - `trips_percent_TAZ_relevant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Percent of TAZ with Potential Impacts from Disruption' is different from the other variable/question types,\n",
    "# so use the below code to generate a chart instead of using the previously defined functions.\n",
    "variabletype = \"percent_TAZ_relevant\"\n",
    "\n",
    "if \"trips_\"+variabletype in summary.columns.to_list():\n",
    "    if (summary['trips_'+variabletype] == 0).all() and (summary['minutespertrip_'+variabletype] == 0).all() and (summary['milespertrip_'+variabletype] == 0).all():\n",
    "        print(\"The '___{}' variable was zero for all categories for trips, minutes per trip, and miles per trip, so no chart was produced.\".format(variabletype))\n",
    "    else:\n",
    "        '''fig = px.bar(summary, x=category_name + '_from', y=\"trips_\"+variabletype,\n",
    "                     labels={category_name + '_from': \"TAZ Indicator Groups Based on Origin TAZ\",\n",
    "                             \"trips_\"+variabletype: \"Percent of TAZ\"},\n",
    "                             title=title)'''\n",
    "        # Establish color list for the plots based on number of categories\n",
    "        if taz_equity[category_name].nunique() == 1:\n",
    "            color_list = color_dict[variabletype][0]\n",
    "        elif taz_equity[category_name].nunique() == 2:\n",
    "            color_list = color_dict[variabletype][0::18]\n",
    "        else:\n",
    "            # Get the estimated interval for systematic sampling of the colors list by dividing 19 (length of list) by the number of unique values (i.e., categories) in the user-supplied variable minus 1.\n",
    "            interval = 19//(taz_equity[category_name].nunique() - 1)\n",
    "            # Take every nth item based on the interval\n",
    "            color_list = color_dict[variabletype][0::interval]\n",
    "        # Now make plots    \n",
    "        fig = go.Figure(go.Bar(\n",
    "            y= summary[\"trips_\"+variabletype],\n",
    "            x= summary[category_name + '_from'],\n",
    "            marker=dict(color=color_list),\n",
    "            hovertemplate=\n",
    "            \"Category of Origin TAZ (\"+category_name+\")\"+\": %{x:.1f}<br>\" +\n",
    "            ylabel_dict[variabletype]+\": \"+\n",
    "            y_hoverformat_dict[variabletype]+\"<br>\" +\n",
    "                \"<extra></extra>\",\n",
    "            text= summary[\"trips_\"+variabletype].map(label_dict[variabletype].format)))\n",
    "        # edit axis labels\n",
    "        fig['layout']['xaxis']['title']=\"Category of Origin TAZ (\"+category_name+\")\"   \n",
    "        fig['layout']['yaxis']['title']=ylabel_dict[variabletype]\n",
    "\n",
    "        # Update title and height\n",
    "        fig.update_layout(title_text=title_dict[variabletype], height=700, showlegend=False)\n",
    "        \n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307da668",
   "metadata": {},
   "source": [
    "### Question 3: What is the projected impact of the resilience investment for this TAZ category?\n",
    "##### Question 3A: What was the absolute impact (change in metric) by TAZ category?\n",
    "- Variables: Overall impact of resilience investment on metrics (i.e., magnitude in the \"resilience\" case minus magnitude in the \"no resilience\" case)\n",
    "    - `trips_delta_absolute`\n",
    "    - `minutespertrip_delta_absolute`\n",
    "    - `milespertrip_delta_absolute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f783e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "makebarcharts(\"delta_absolute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258ab8b",
   "metadata": {},
   "source": [
    "##### Question 3B: What was the relative impact (change in metric expressed as a percentage of the \"no resilience\" magnitude) by TAZ category?\n",
    "- Variables: Same as the above set, except divided by the magnitude in the \"no resilience\" case and multiplied by 100 to show percent change relative to \"no resilience\" case\n",
    "    - `trips_delta_relative`\n",
    "    - `minutespertrip_delta_relative`\n",
    "    - `milespertrip_delta_relative`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01300e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "makebarcharts(\"delta_relative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021cf29",
   "metadata": {},
   "source": [
    "# Statistically Significant Differences between Groups\n",
    "## Context\n",
    "The charts above show differences in various metrics among the groups (where each \"group\" is formed based on the category of the origin TAZ). Ultimately, to understand whether the benefits of the resilience investment are evenly distributed across these groups, one key question is whether the proportions of impacted trips in the \"resilience\" case deviate significantly from the expected proportions, where our expectation is based on the corresponding proportions in the \"no resilience\" case. For example, if a disruption event is equally relevant for two groups, such that 50 percent of the disrupted trips are in group \"0\" and the other 50 percent are in group \"1\" (without the resilience investment), then we we would also expect to see approximately 50 percent of disrupted trips for group \"0\" and 50 percent for group \"1\" if the same disruption event were to occur *with* the resilience investment. If the proportions are significantly different from expected, this may suggest that the benefits of the resilience investment are not proportionally distributed. If instead the proportions were 70 percent and 30 percent in the \"no resilience\" case, then we would similarly expect to see approximately 70 percent of impacted trips in group \"0\" and 30 percent in group \"1\" in the resilience case.\n",
    "\n",
    "The determination of whether the differences from \"expected\" are \"significant\" is based on a chi square test using the p-value that the user provides in the TAZ metrics configuration file (`TAZ_metrics.config` has 0.05 by default). For example, if the p-value is 0.05 (i.e. 5 percent), and the test shows a difference with a p-value less than 0.05, then we can state that there is a 95 percent chance that there is actually a difference and a 5 percent chance that any deviation was observed through random chance. The output below shows the results specific to this analysis.\n",
    "\n",
    "## Results Specific to this Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct chi-square test\n",
    "\n",
    "# Expected and observed values for trips\n",
    "\n",
    "# Calculate the proportion of all disrupted trips in the \"NO RESILIENCE\" case within each equity category.\n",
    "# Expect the proportion of all disrupted trips in the \"RESILIENCE\" case in each equity category to be the same,\n",
    "# assuming that the resilience investment has equitable benefit for each category.\n",
    "f_exp = summary['trips_disrupt_noresil']/summary['trips_disrupt_noresil'].sum()\n",
    "\n",
    "# The actual observed proportions of disrupted trips in \"RESILIENCE\" case are: \n",
    "f_obs = summary['trips_disrupt_resil']/summary['trips_disrupt_resil'].sum()\n",
    "\n",
    "# Chi-square test\n",
    "teststat,testp = chisquare(f_obs=f_obs, f_exp=f_exp)\n",
    "testp = round(testp, 5)\n",
    "\n",
    "# p-value is derived from the configuration file. The default is 0.05\n",
    "# Check whether the p-value that resulted from the chi square test is less than the p-value in the configuration file\n",
    "if testp < pval:\n",
    "    print(\"The proportions of disrupted trips in the 'resilience case' differed significantly from expected\\nwith a p-value of {}, suggesting that the benefits of resilience may not be distributed evenly.\".format(testp))\n",
    "    print(\"The chi square statistic (a measure of the difference between the observed and expected proportions)\\nwas {} with a p-value of {}, which is less than the user-supplied p-value of {}.\".format(teststat, testp, pval))\n",
    "else:\n",
    "    print(\"The proportions of disrupted trips in the 'resilience case' did not differ significantly from expected, suggesting that the resilience benefits may be evenly distributed.\")\n",
    "    print(\"The p-value resulting from the chi square test was {}, which is greater than the user-supplied p-value of {}.\".format(testp, pval))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RDRenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
